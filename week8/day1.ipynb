{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc0e1c1c-be6a-4395-bbbd-eeafc9330d7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import locale\n",
    "import modal\n",
    "from agents.preprocessor import Preprocessor\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "765bb54b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cp1252\n"
     ]
    }
   ],
   "source": [
    "# To check that your computer can output special characters, make sure this outputs UTF-8 \n",
    "\n",
    "print(locale.getpreferredencoding())  # Should print 'UTF-8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b66f056",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PYTHONIOENCODING\"] = \"utf-8\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5c8533-9f66-448f-b9b2-133d1ff50639",
   "metadata": {},
   "source": [
    "# Setting up the modal tokens\n",
    "\n",
    "## IMPORTANT - please do read and follow these instructions!\n",
    "\n",
    "First please visit: https://modal.com\n",
    "\n",
    "And sign up for an account. Then click the Avatar menu on the top right and select \"Settings\"\n",
    "\n",
    "Then click \"API Tokens\" in the left sidebar, then click \"New Token\".\n",
    "\n",
    "You will be given something like this to run:\n",
    "\n",
    "`modal token set --token-id ak-somethinghere --token-secret as-somethinghere`\n",
    "\n",
    "But because we're using uv, the real thing to run is this:\n",
    "\n",
    "`uv run modal token set --token-id ak-somethinghere --token-secret as-somethinghere`\n",
    "\n",
    "### Troubleshooting\n",
    "\n",
    "If you have problems, 3 things to try:\n",
    "\n",
    "1. Try running `uv run modal token new` before the `uv run modal token set..`  \n",
    "\n",
    "2. Suggestion from student David S. on Windows:\n",
    "\n",
    "> In case anyone else using Windows hits this problem: Along with having to run `modal token new` from a command prompt, you have to move the generated token file. It will deploy the token file (.modal.toml) to your Windows profile folder. The virtual environment couldn't see that location (strangely, it couldn't even after I set environment variables for it and rebooted). I moved that token file to the folder I'm operating out of for the lab and it stopped throwing auth errors.\n",
    "\n",
    "3. Doing in the manual way:\n",
    "\n",
    "It might be totally fine to simply add the 2 keys directly to your .env file:\n",
    "\n",
    "```\n",
    "MODAL_TOKEN_ID=ak-...\n",
    "MODAL_TOKEN_SECRET=as-...\n",
    "```\n",
    "\n",
    "Then rerun `load_dotenv(override=True)` to load these environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b133701-f550-44a1-a67f-eb7ccc4769a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hello import app, hello, hello_europe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f3f73ae-1295-49f3-9099-b8b41fc3429b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello from Hyderabad, Telangana, IN!!'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with app.run():\n",
    "    reply=hello.local()\n",
    "reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1d8c6f9-edc7-4e52-9b3a-c07d7cff1ac7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello from Chicago, Illinois, US!!'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with app.run():\n",
    "    reply=hello.remote()\n",
    "reply"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c075e9-49c7-4ebd-812f-83196d32de32",
   "metadata": {},
   "source": [
    "## Added thanks to student Tue H.\n",
    "\n",
    "If you look in hello.py, I've added a simple function hello_europe\n",
    "\n",
    "That uses the decorator:  \n",
    "`@app.function(image=image, region=\"eu\")`\n",
    "\n",
    "See the result below! More region specific settings are [here](https://modal.com/docs/guide/region-selection)\n",
    "\n",
    "Note that it does consume marginally more credits to specify a region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b027da1a-c79d-42cb-810d-32ddca31aa02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello from Brussels, Brussels Capital, BE!!'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with app.run():\n",
    "    reply=hello_europe.remote()\n",
    "reply"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e8d804-c027-45fb-8fef-06e7bba6295a",
   "metadata": {},
   "source": [
    "# Before we move on -\n",
    "\n",
    "## We need to set your HuggingFace Token as a secret in Modal\n",
    "\n",
    "## Super important - please read - this confuses a lot of people!\n",
    "\n",
    "Secrets in Modal are given a **name** that describes the secret.  \n",
    "Then the secret itself has a KEY and a VALUE.  \n",
    "We will be setting up a secret with:  \n",
    "\n",
    "Name: huggingface-secret  \n",
    "Key: HF_TOKEN  \n",
    "Value: hf_...  \n",
    "\n",
    "## The bulletproof recipe:\n",
    "\n",
    "1. Go to modal.com, sign in and go to your dashboard  \n",
    "2. Click on Secrets in the nav bar  \n",
    "3. Create new secret, click on Hugging Face, this new secret needs to be called **huggingface-secret** because that's how we refer to it in the code  \n",
    "4. Fill in your key as HF_TOKEN and the value as your actual token hf_...  \n",
    "5. Click done\n",
    "\n",
    "### And now back to business: time to work with Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb8b6c41-8259-4329-b1c4-a1f67d26d1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This import may give a deprecation warning about adding local Python modules to the Image\n",
    "# That warning can be safely ignored. You may get the same warning in other places, too..\n",
    "\n",
    "from llama import app, generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db4a718a-d95d-4f61-9688-c9df21d88fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[32mâœ“\u001b[0m Initialized. \u001b[37mView run at \u001b[0m\n",
      "\u001b[4;37mhttps://modal.com/apps/preetam-naik3/main/ap-S7LGHzX5blHrFkLxO63pFN\u001b[0m\n",
      "\u001b[2K\u001b[34m-\u001b[0m Initializing...\n",
      "\u001b[2K\u001b[34m\\\u001b[0m Creating objects...objects...\n",
      "\u001b[90mâ””â”€â”€ \u001b[0m\u001b[34m-\u001b[0m Creating mount c:\\Users\\pritam\\Desktop\\llm_engineering\\week8\\llama.py: \n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m Creating objects...\n",
      "\u001b[90mâ””â”€â”€ \u001b[0m\u001b[34m|\u001b[0m Creating mount c:\\Users\\pritam\\Desktop\\llm_engineering\\week8\\llama.py: \n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m Creating objects...\n",
      "\u001b[90mâ””â”€â”€ \u001b[0m\u001b[34m/\u001b[0m Creating mount c:\\Users\\pritam\\Desktop\\llm_engineering\\week8\\llama.py: \n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m Creating objects...\n",
      "\u001b[90mâ””â”€â”€ \u001b[0m\u001b[34m\\\u001b[0m Creating mount c:\\Users\\pritam\\Desktop\\llm_engineering\\week8\\llama.py: \n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m Creating objects...\n",
      "\u001b[90mâ””â”€â”€ \u001b[0m\u001b[34m/\u001b[0m Creating mount c:\\Users\\pritam\\Desktop\\llm_engineering\\week8\\llama.py: \n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m Creating objects...\n",
      "\u001b[90mâ””â”€â”€ \u001b[0m\u001b[34m\\\u001b[0m Creating mount c:\\Users\\pritam\\Desktop\\llm_engineering\\week8\\llama.py: \n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m Creating objects...\n",
      "\u001b[90mâ””â”€â”€ \u001b[0m\u001b[34m/\u001b[0m Creating mount c:\\Users\\pritam\\Desktop\\llm_engineering\\week8\\llama.py: \n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m Creating objects...\n",
      "\u001b[90mâ””â”€â”€ \u001b[0m\u001b[34m\\\u001b[0m Creating mount c:\\Users\\pritam\\Desktop\\llm_engineering\\week8\\llama.py: \n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m Creating objects...\n",
      "\u001b[90mâ””â”€â”€ \u001b[0m\u001b[34m/\u001b[0m Creating mount c:\\Users\\pritam\\Desktop\\llm_engineering\\week8\\llama.py: \n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m Creating objects...\n",
      "\u001b[90mâ”œâ”€â”€ \u001b[0mðŸ”¨ Created mount c:\\Users\\pritam\\Desktop\\llm_engineering\\week8\\llama.py\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m Creating objects......\n",
      "\u001b[90mâ”œâ”€â”€ \u001b[0mðŸ”¨ Created mount c:\\Users\\pritam\\Desktop\\llm_engineering\\week8\\llama.py\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[34m|\u001b[0m Creating objects......\n",
      "\u001b[90mâ”œâ”€â”€ \u001b[0mðŸ”¨ Created mount c:\\Users\\pritam\\Desktop\\llm_engineering\\week8\\llama.py\n",
      "\u001b[90mâ””â”€â”€ \u001b[0mðŸ”¨ Created function generate.\n",
      "\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[32mâœ“\u001b[0m Created objects.\n",
      "\u001b[90mâ”œâ”€â”€ \u001b[0mðŸ”¨ Created mount c:\\Users\\pritam\\Desktop\\llm_engineering\\week8\\llama.py\n",
      "\u001b[90mâ””â”€â”€ \u001b[0mðŸ”¨ Created function generate.\n",
      "\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mWorker assigned...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mWorker assigned...\u001b[0m \u001b[37mView app at \u001b[0mFN\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning...\u001b[0m \u001b[37mView app at \u001b[0mFkLxO63pFN\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mLoading images (1 containers initializing)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m|\u001b[0m \u001b[34mLoading images (1 containers initializing)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mLoading images (1 containers initializing)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mLoading images (1 containers initializing)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mLoading images (1 containers initializing)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mLoading images (1 containers initializing)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mLoading images (1 containers initializing)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m|\u001b[0m \u001b[34mLoading images (1 containers initializing)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m-\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m|\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m|\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m-\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m|\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m-\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m|\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m-\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m|\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m-\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m|\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m-\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m|\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m-\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m|\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m-\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m|\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m-\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m|\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m-\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m|\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m-\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[31mLoading weights:   0%|          | 0/201 [00:00<?, ?it/s]Loading weights:   0%|          | 1/201 [00:00<00:00, 14614.30it/s, Materializing param=lm_head.weight]Loading weights:   0%|          | 1/201 [00:00<00:00, 5949.37it/s, Materializing param=lm_head.weight] Loading weights:   1%|          | 2/201 [00:00<00:00, 424.74it/s, Materializing param=model.embed_tokens.weight]\u001b[0m\n",
      "\u001b[2K\u001b[34m|\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0mew app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/preetam\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1ALoading weights:   1%|          | 2/201 [00:00<00:00, 405.36it/s, Materializing param=model.embed_tokens.weight]Loading weights:   1%|â–         | 3/201 [00:00<00:02, 74.01it/s, Materializing param=model.layers.0.input_layernorm.weight]Loading weights:   1%|â–         | 3/201 [00:00<00:02, 73.39it/s, Materializing param=model.layers.0.input_layernorm.weight]Loading weights:   2%|â–         | 4/201 [00:00<00:02, 97.06it/s, Materializing param=model.layers.0.mlp.down_proj.weight]  \u001b[0m\n",
      "\u001b[2K\u001b[31m\u001b[1ALoading weights:   2%|â–         | 4/201 [00:00<00:02, 96.59it/s, Materializing param=model.layers.0.mlp.down_proj.weight]Loading weights:   2%|â–         | 5/201 [00:00<00:01, 106.73it/s, Materializing param=model.layers.0.mlp.gate_proj.weight]\u001b[0m\n",
      "\u001b[2K\u001b[31m\u001b[1ALoading weights:   2%|â–         | 5/201 [00:00<00:01, 106.26it/s, Materializing param=model.layers.0.mlp.gate_proj.weight]Loading weights:   3%|â–Ž         | 6/201 [00:00<00:01, 112.54it/s, Materializing param=model.layers.0.mlp.up_proj.weight]  \u001b[0m\n",
      "\u001b[2K\u001b[31m\u001b[1ALoading weights:   3%|â–Ž         | 6/201 [00:00<00:01, 112.13it/s, Materializing param=model.layers.0.mlp.up_proj.weight]Loading weights:   3%|â–Ž         | 7/201 [00:00<00:01, 117.11it/s, Materializing param=model.layers.0.post_attention_layernorm.weight]Loading weights:   3%|â–Ž         | 7/201 [00:00<00:01, 116.62it/s, Materializing param=model.layers.0.post_attention_layernorm.weight]Loading weights:   4%|â–         | 8/201 [00:00<00:01, 132.00it/s, Materializing param=model.layers.0.self_attn.k_proj.weight]        Loading weights:   4%|â–         | 8/201 [00:00<00:01, 131.64it/s, Materializing param=model.layers.0.self_attn.k_proj.weight]Loading weights:   4%|â–         | 9/201 [00:00<00:01, 146.79it/s, Materializing param=model.layers.0.self_attn.o_proj.weight]Loading weights:   4%|â–         | 9/201 [00:00<00:01, 146.45it/s, Materializing param=model.layers.0.self_attn.o_proj.weight]Loading weights:   5%|â–         | 10/201 [00:00<00:01, 160.91it/s, Materializing param=model.layers.0.self_attn.q_proj.weight]Loading weights:   5%|â–         | 10/201 [00:00<00:01, 160.47it/s, Materializing param=model.layers.0.self_attn.q_proj.weight]Loading weights:   5%|â–Œ         | 11/201 [00:00<00:01, 170.49it/s, Materializing param=model.layers.0.self_attn.v_proj.weight]Loading weights:   5%|â–Œ         | 11/201 [00:00<00:01, 170.15it/s, Materializing param=model.layers.0.self_attn.v_proj.weight]Loading weights:   6%|â–Œ         | 12/201 [00:00<00:01, 183.52it/s, Materializing param=model.layers.1.input_layernorm.weight] \u001b[0m\n",
      "\u001b[2K\u001b[31m\u001b[1ALoading weights:   6%|â–Œ         | 12/201 [00:00<00:01, 183.24it/s, Materializing param=model.layers.1.input_layernorm.weight]Loading weights:   6%|â–‹         | 13/201 [00:00<00:01, 165.77it/s, Materializing param=model.layers.1.mlp.down_proj.weight]  Loading weights:   6%|â–‹         | 13/201 [00:00<00:01, 165.28it/s, Materializing param=model.layers.1.mlp.down_proj.weight]Loading weights:   7%|â–‹         | 14/201 [00:00<00:01, 177.20it/s, Materializing param=model.layers.1.mlp.gate_proj.weight]Loading weights:   7%|â–‹         | 14/201 [00:00<00:01, 176.94it/s, Materializing param=model.layers.1.mlp.gate_proj.weight]Loading weights:   7%|â–‹         | 15/201 [00:00<00:00, 189.20it/s, Materializing param=model.layers.1.mlp.up_proj.weight]  Loading weights:   7%|â–‹         | 15/201 [00:00<00:00, 189.02it/s, Materializing param=model.layers.1.mlp.up_proj.weight]\u001b[0m\n",
      "\u001b[2K\u001b[31m\u001b[1ALoading weights:   8%|â–Š         | 16/201 [00:00<00:00, 189.16it/s, Materializing param=model.layers.1.post_attention_layernorm.weight]Loading weights:   8%|â–Š         | 16/201 [00:00<00:00, 188.62it/s, Materializing param=model.layers.1.post_attention_layernorm.weight]Loading weights:   8%|â–Š         | 17/201 [00:00<00:00, 199.27it/s, Materializing param=model.layers.1.self_attn.k_proj.weight]        Loading weights:   8%|â–Š         | 17/201 [00:00<00:00, 198.71it/s, Materializing param=model.layers.1.self_attn.k_proj.weight]Loading weights:   9%|â–‰         | 18/201 [00:00<00:00, 209.81it/s, Materializing param=model.layers.1.self_attn.o_proj.weight]Loading weights:   9%|â–‰         | 18/201 [00:00<00:00, 209.46it/s, Materializing param=model.layers.1.self_attn.o_proj.weight]Loading weights:   9%|â–‰         | 19/201 [00:00<00:00, 218.92it/s, Materializing param=model.layers.1.self_attn.q_proj.weight]Loading weights:   9%|â–‰         | 19/201 [00:00<00:00, 218.50it/s, Materializing param=model.layers.1.self_attn.q_proj.weight]Loading weights:  10%|â–‰         | 20/201 [00:00<00:00, 224.33it/s, Materializing param=model.layers.1.self_attn.v_proj.weight]Loading weights:  10%|â–‰         | 20/201 [00:00<00:00, 223.93it/s, Materializing param=model.layers.1.self_attn.v_proj.weight]Loading weights:  10%|â–ˆ         | 21/201 [00:00<00:00, 232.94it/s, Materializing param=model.layers.2.input_layernorm.weight] Loading weights:  10%|â–ˆ         | 21/201 [00:00<00:00, 232.49it/s, Materializing param=model.layers.2.input_layernorm.weight]Loading weights:  11%|â–ˆ         | 22/201 [00:00<00:00, 242.80it/s, Materializing param=model.layers.2.mlp.down_proj.weight]  \u001b[0m\n",
      "\u001b[2K\u001b[31m\u001b[1ALoading weights:  11%|â–ˆ         | 22/201 [00:00<00:00, 242.42it/s, Materializing param=model.layers.2.mlp.down_proj.weight]Loading weights:  11%|â–ˆâ–        | 23/201 [00:00<00:00, 237.82it/s, Materializing param=model.layers.2.mlp.gate_proj.weight]Loading weights:  11%|â–ˆâ–        | 23/201 [00:00<00:00, 237.18it/s, Materializing param=model.layers.2.mlp.gate_proj.weight]Loading weights:  12%|â–ˆâ–        | 24/201 [00:00<00:00, 234.44it/s, Materializing param=model.layers.2.mlp.gate_proj.weight]Loading weights:  12%|â–ˆâ–        | 24/201 [00:00<00:00, 234.44it/s, Materializing param=model.layers.2.mlp.up_proj.weight]  \u001b[0m\n",
      "\u001b[2K\u001b[31m\u001b[1ALoading weights:  12%|â–ˆâ–        | 24/201 [00:00<00:00, 234.44it/s, Materializing param=model.layers.2.mlp.up_proj.weight]Loading weights:  12%|â–ˆâ–        | 25/201 [00:00<00:00, 234.44it/s, Materializing param=model.layers.2.post_attention_layernorm.weight]Loading weights:  12%|â–ˆâ–        | 25/201 [00:00<00:00, 234.44it/s, Materializing param=model.layers.2.post_attention_layernorm.weight]Loading weights:  13%|â–ˆâ–Ž        | 26/201 [00:00<00:00, 234.44it/s, Materializing param=model.layers.2.self_attn.k_proj.weight]        Loading weights:  13%|â–ˆâ–Ž        | 26/201 [00:00<00:00, 234.44it/s, Materializing param=model.layers.2.self_attn.k_proj.weight]Loading weights:  13%|â–ˆâ–Ž        | 27/201 [00:00<00:00, 234.44it/s, Materializing param=model.layers.2.self_attn.o_proj.weight]Loading weights:  13%|â–ˆâ–Ž        | 27/201 [00:00<00:00, 234.44it/s, Materializing param=model.layers.2.self_attn.o_proj.weight]Loading weights:  14%|â–ˆâ–        | 28/201 [00:00<00:00, 234.44it/s, Materializing param=model.layers.2.self_attn.q_proj.weight]\u001b[0m\n",
      "\u001b[2K\u001b[31m\u001b[1ALoading weights:  14%|â–ˆâ–        | 28/201 [00:00<00:00, 234.44it/s, Materializing param=model.layers.2.self_attn.q_proj.weight]Loading weights:  14%|â–ˆâ–        | 29/201 [00:00<00:00, 234.44it/s, Materializing param=model.layers.2.self_attn.v_proj.weight]Loading weights:  14%|â–ˆâ–        | 29/201 [00:00<00:00, 234.44it/s, Materializing param=model.layers.2.self_attn.v_proj.weight]Loading weights:  15%|â–ˆâ–        | 30/201 [00:00<00:00, 234.44it/s, Materializing param=model.layers.3.input_layernorm.weight] Loading weights:  15%|â–ˆâ–        | 30/201 [00:00<00:00, 234.44it/s, Materializing param=model.layers.3.input_layernorm.weight]Loading weights:  15%|â–ˆâ–Œ        | 31/201 [00:00<00:00, 234.44it/s, Materializing param=model.layers.3.mlp.down_proj.weight]  \u001b[0m\n",
      "\u001b[2K\u001b[31m\u001b[1ALoading weights:  15%|â–ˆâ–Œ        | 31/201 [00:00<00:00, 234.44it/s, Materializing param=model.layers.3.mlp.down_proj.weight]Loading weights:  16%|â–ˆâ–Œ        | 32/201 [00:00<00:00, 234.44it/s, Materializing param=model.layers.3.mlp.gate_proj.weight]\u001b[0m\n",
      "\u001b[2K\u001b[31m\u001b[1ALoading weights:  16%|â–ˆâ–Œ        | 32/201 [00:00<00:00, 234.44it/s, Materializing param=model.layers.3.mlp.gate_proj.weight]Loading weights:  16%|â–ˆâ–‹        | 33/201 [00:00<00:00, 234.44it/s, Materializing param=model.layers.3.mlp.up_proj.weight]  \u001b[0m\n",
      "\u001b[2K\u001b[31m\u001b[1ALoading weights:  16%|â–ˆâ–‹        | 33/201 [00:00<00:00, 234.44it/s, Materializing param=model.layers.3.mlp.up_proj.weight]Loading weights:  17%|â–ˆâ–‹        | 34/201 [00:00<00:00, 234.44it/s, Materializing param=model.layers.3.post_attention_layernorm.weight]Loading weights:  17%|â–ˆâ–‹        | 34/201 [00:00<00:00, 234.44it/s, Materializing param=model.layers.3.post_attention_layernorm.weight]Loading weights:  17%|â–ˆâ–‹        | 35/201 [00:00<00:00, 234.44it/s, Materializing param=model.layers.3.self_attn.k_proj.weight]        Loading weights:  17%|â–ˆâ–‹        | 35/201 [00:00<00:00, 234.44it/s, Materializing param=model.layers.3.self_attn.k_proj.weight]Loading weights:  18%|â–ˆâ–Š        | 36/201 [00:00<00:00, 234.44it/s, Materializing param=model.layers.3.self_attn.o_proj.weight]Loading weights:  18%|â–ˆâ–Š        | 36/201 [00:00<00:00, 234.44it/s, Materializing param=model.layers.3.self_attn.o_proj.weight]Loading weights:  18%|â–ˆâ–Š        | 37/201 [00:00<00:00, 234.44it/s, Materializing param=model.layers.3.self_attn.q_proj.weight]Loading weights:  18%|â–ˆâ–Š        | 37/201 [00:00<00:00, 234.44it/s, Materializing param=model.layers.3.self_attn.q_proj.weight]Loading weights:  19%|â–ˆâ–‰        | 38/201 [00:00<00:00, 234.44it/s, Materializing param=model.layers.3.self_attn.v_proj.weight]Loading weights:  19%|â–ˆâ–‰        | 38/201 [00:00<00:00, 234.44it/s, Materializing param=model.layers.3.self_attn.v_proj.weight]Loading weights:  19%|â–ˆâ–‰        | 39/201 [00:00<00:00, 234.44it/s, Materializing param=model.layers.4.input_layernorm.weight] \u001b[0m\n",
      "\u001b[2K\u001b[31m\u001b[1ALoading weights:  19%|â–ˆâ–‰        | 39/201 [00:00<00:00, 234.44it/s, Materializing param=model.layers.4.input_layernorm.weight]Loading weights:  20%|â–ˆâ–‰        | 40/201 [00:00<00:00, 234.44it/s, Materializing param=model.layers.4.mlp.down_proj.weight]  Loading weights:  20%|â–ˆâ–‰        | 40/201 [00:00<00:00, 234.44it/s, Materializing param=model.layers.4.mlp.down_proj.weight]Loading weights:  20%|â–ˆâ–ˆ        | 41/201 [00:00<00:00, 234.44it/s, Materializing param=model.layers.4.mlp.gate_proj.weight]\u001b[0m\n",
      "\u001b[2K\u001b[31m\u001b[1ALoading weights:  20%|â–ˆâ–ˆ        | 41/201 [00:00<00:00, 234.44it/s, Materializing param=model.layers.4.mlp.gate_proj.weight]Loading weights:  21%|â–ˆâ–ˆ        | 42/201 [00:00<00:00, 234.44it/s, Materializing param=model.layers.4.mlp.up_proj.weight]  \u001b[0m\n",
      "\u001b[2K\u001b[31m\u001b[1ALoading weights:  21%|â–ˆâ–ˆ        | 42/201 [00:00<00:00, 234.44it/s, Materializing param=model.layers.4.mlp.up_proj.weight]Loading weights:  21%|â–ˆâ–ˆâ–       | 43/201 [00:00<00:00, 234.44it/s, Materializing param=model.layers.4.post_attention_layernorm.weight]Loading weights:  21%|â–ˆâ–ˆâ–       | 43/201 [00:00<00:00, 234.44it/s, Materializing param=model.layers.4.post_attention_layernorm.weight]Loading weights:  22%|â–ˆâ–ˆâ–       | 44/201 [00:00<00:00, 234.44it/s, Materializing param=model.layers.4.self_attn.k_proj.weight]        Loading weights:  22%|â–ˆâ–ˆâ–       | 44/201 [00:00<00:00, 234.44it/s, Materializing param=model.layers.4.self_attn.k_proj.weight]Loading weights:  22%|â–ˆâ–ˆâ–       | 45/201 [00:00<00:00, 234.44it/s, Materializing param=model.layers.4.self_attn.o_proj.weight]Loading weights:  22%|â–ˆâ–ˆâ–       | 45/201 [00:00<00:00, 234.44it/s, Materializing param=model.layers.4.self_attn.o_proj.weight]Loading weights:  23%|â–ˆâ–ˆâ–Ž       | 46/201 [00:00<00:00, 234.44it/s, Materializing param=model.layers.4.self_attn.q_proj.weight]Loading weights:  23%|â–ˆâ–ˆâ–Ž       | 46/201 [00:00<00:00, 234.44it/s, Materializing param=model.layers.4.self_attn.q_proj.weight]Loading weights:  23%|â–ˆâ–ˆâ–Ž       | 47/201 [00:00<00:00, 234.44it/s, Materializing param=model.layers.4.self_attn.v_proj.weight]Loading weights:  23%|â–ˆâ–ˆâ–Ž       | 47/201 [00:00<00:00, 234.44it/s, Materializing param=model.layers.4.self_attn.v_proj.weight]Loading weights:  24%|â–ˆâ–ˆâ–       | 48/201 [00:00<00:00, 234.44it/s, Materializing param=model.layers.5.input_layernorm.weight] Loading weights:  24%|â–ˆâ–ˆâ–       | 48/201 [00:00<00:00, 234.44it/s, Materializing param=model.layers.5.input_layernorm.weight]Loading weights:  24%|â–ˆâ–ˆâ–       | 49/201 [00:00<00:00, 234.44it/s, Materializing param=model.layers.5.mlp.down_proj.weight]  \u001b[0m\n",
      "\u001b[2K\u001b[31m\u001b[1ALoading weights:  24%|â–ˆâ–ˆâ–       | 49/201 [00:00<00:00, 234.44it/s, Materializing param=model.layers.5.mlp.down_proj.weight]Loading weights:  25%|â–ˆâ–ˆâ–       | 50/201 [00:00<00:00, 234.44it/s, Materializing param=model.layers.5.mlp.gate_proj.weight]\u001b[0m\n",
      "\u001b[2K\u001b[31m\u001b[1ALoading weights:  25%|â–ˆâ–ˆâ–       | 50/201 [00:00<00:00, 234.44it/s, Materializing param=model.layers.5.mlp.gate_proj.weight]Loading weights:  25%|â–ˆâ–ˆâ–Œ       | 51/201 [00:00<00:00, 234.44it/s, Materializing param=model.layers.5.mlp.up_proj.weight]  \u001b[0m\n",
      "\u001b[2K\u001b[31m\u001b[1ALoading weights:  25%|â–ˆâ–ˆâ–Œ       | 51/201 [00:00<00:00, 234.44it/s, Materializing param=model.layers.5.mlp.up_proj.weight]Loading weights:  26%|â–ˆâ–ˆâ–Œ       | 52/201 [00:00<00:00, 234.44it/s, Materializing param=model.layers.5.post_attention_layernorm.weight]Loading weights:  26%|â–ˆâ–ˆâ–Œ       | 52/201 [00:00<00:00, 234.44it/s, Materializing param=model.layers.5.post_attention_layernorm.weight]Loading weights:  26%|â–ˆâ–ˆâ–‹       | 53/201 [00:00<00:00, 234.44it/s, Materializing param=model.layers.5.self_attn.k_proj.weight]        Loading weights:  26%|â–ˆâ–ˆâ–‹       | 53/201 [00:00<00:00, 234.44it/s, Materializing param=model.layers.5.self_attn.k_proj.weight]Loading weights:  27%|â–ˆâ–ˆâ–‹       | 54/201 [00:00<00:00, 234.44it/s, Materializing param=model.layers.5.self_attn.o_proj.weight]Loading weights:  27%|â–ˆâ–ˆâ–‹       | 54/201 [00:00<00:00, 234.44it/s, Materializing param=model.layers.5.self_attn.o_proj.weight]Loading weights:  27%|â–ˆâ–ˆâ–‹       | 55/201 [00:00<00:00, 234.44it/s, Materializing param=model.layers.5.self_attn.q_proj.weight]Loading weights:  27%|â–ˆâ–ˆâ–‹       | 55/201 [00:00<00:00, 234.44it/s, Materializing param=model.layers.5.self_attn.q_proj.weight]Loading weights:  28%|â–ˆâ–ˆâ–Š       | 56/201 [00:00<00:00, 234.44it/s, Materializing param=model.layers.5.self_attn.v_proj.weight]\u001b[0m\n",
      "\u001b[2K\u001b[31m\u001b[1ALoading weights:  28%|â–ˆâ–ˆâ–Š       | 56/201 [00:00<00:00, 234.44it/s, Materializing param=model.layers.5.self_attn.v_proj.weight]Loading weights:  28%|â–ˆâ–ˆâ–Š       | 57/201 [00:00<00:00, 234.44it/s, Materializing param=model.layers.6.input_layernorm.weight] Loading weights:  28%|â–ˆâ–ˆâ–Š       | 57/201 [00:00<00:00, 234.44it/s, Materializing param=model.layers.6.input_layernorm.weight]Loading weights:  29%|â–ˆâ–ˆâ–‰       | 58/201 [00:00<00:00, 234.44it/s, Materializing param=model.layers.6.mlp.down_proj.weight]  \u001b[0m\n",
      "\u001b[2K\u001b[31m\u001b[1ALoading weights:  29%|â–ˆâ–ˆâ–‰       | 58/201 [00:00<00:00, 234.44it/s, Materializing param=model.layers.6.mlp.down_proj.weight]Loading weights:  29%|â–ˆâ–ˆâ–‰       | 59/201 [00:00<00:00, 234.44it/s, Materializing param=model.layers.6.mlp.gate_proj.weight]\u001b[0m\n",
      "\u001b[2K\u001b[31m\u001b[1ALoading weights:  29%|â–ˆâ–ˆâ–‰       | 59/201 [00:00<00:00, 234.44it/s, Materializing param=model.layers.6.mlp.gate_proj.weight]Loading weights:  30%|â–ˆâ–ˆâ–‰       | 60/201 [00:00<00:00, 297.84it/s, Materializing param=model.layers.6.mlp.gate_proj.weight]Loading weights:  30%|â–ˆâ–ˆâ–‰       | 60/201 [00:00<00:00, 297.84it/s, Materializing param=model.layers.6.mlp.up_proj.weight]  Loading weights:  30%|â–ˆâ–ˆâ–‰       | 60/201 [00:00<00:00, 297.84it/s, Materializing param=model.layers.6.mlp.up_proj.weight]Loading weights:  30%|â–ˆâ–ˆâ–ˆ       | 61/201 [00:00<00:00, 297.84it/s, Materializing param=model.layers.6.post_attention_layernorm.weight]Loading weights:  30%|â–ˆâ–ˆâ–ˆ       | 61/201 [00:00<00:00, 297.84it/s, Materializing param=model.layers.6.post_attention_layernorm.weight]Loading weights:  31%|â–ˆâ–ˆâ–ˆ       | 62/201 [00:00<00:00, 297.84it/s, Materializing param=model.layers.6.self_attn.k_proj.weight]        Loading weights:  31%|â–ˆâ–ˆâ–ˆ       | 62/201 [00:00<00:00, 297.84it/s, Materializing param=model.layers.6.self_attn.k_proj.weight]Loading weights:  31%|â–ˆâ–ˆâ–ˆâ–      | 63/201 [00:00<00:00, 297.84it/s, Materializing param=model.layers.6.self_attn.o_proj.weight]Loading weights:  31%|â–ˆâ–ˆâ–ˆâ–      | 63/201 [00:00<00:00, 297.84it/s, Materializing param=model.layers.6.self_attn.o_proj.weight]Loading weights:  32%|â–ˆâ–ˆâ–ˆâ–      | 64/201 [00:00<00:00, 297.84it/s, Materializing param=model.layers.6.self_attn.q_proj.weight]Loading weights:  32%|â–ˆâ–ˆâ–ˆâ–      | 64/201 [00:00<00:00, 297.84it/s, Materializing param=model.layers.6.self_attn.q_proj.weight]Loading weights:  32%|â–ˆâ–ˆâ–ˆâ–      | 65/201 [00:00<00:00, 297.84it/s, Materializing param=model.layers.6.self_attn.v_proj.weight]\u001b[0m\n",
      "\u001b[2K\u001b[31m\u001b[1ALoading weights:  32%|â–ˆâ–ˆâ–ˆâ–      | 65/201 [00:00<00:00, 297.84it/s, Materializing param=model.layers.6.self_attn.v_proj.weight]Loading weights:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 66/201 [00:00<00:00, 297.84it/s, Materializing param=model.layers.7.input_layernorm.weight] Loading weights:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 66/201 [00:00<00:00, 297.84it/s, Materializing param=model.layers.7.input_layernorm.weight]Loading weights:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 67/201 [00:00<00:00, 297.84it/s, Materializing param=model.layers.7.mlp.down_proj.weight]  Loading weights:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 67/201 [00:00<00:00, 297.84it/s, Materializing param=model.layers.7.mlp.down_proj.weight]Loading weights:  34%|â–ˆâ–ˆâ–ˆâ–      | 68/201 [00:00<00:00, 297.84it/s, Materializing param=model.layers.7.mlp.gate_proj.weight]\u001b[0m\n",
      "\u001b[2K\u001b[31m\u001b[1ALoading weights:  34%|â–ˆâ–ˆâ–ˆâ–      | 68/201 [00:00<00:00, 297.84it/s, Materializing param=model.layers.7.mlp.gate_proj.weight]Loading weights:  34%|â–ˆâ–ˆâ–ˆâ–      | 69/201 [00:00<00:00, 297.84it/s, Materializing param=model.layers.7.mlp.up_proj.weight]  \u001b[0m\n",
      "\u001b[2K\u001b[31m\u001b[1ALoading weights:  34%|â–ˆâ–ˆâ–ˆâ–      | 69/201 [00:00<00:00, 297.84it/s, Materializing param=model.layers.7.mlp.up_proj.weight]Loading weights:  35%|â–ˆâ–ˆâ–ˆâ–      | 70/201 [00:00<00:00, 297.84it/s, Materializing param=model.layers.7.post_attention_layernorm.weight]Loading weights:  35%|â–ˆâ–ˆâ–ˆâ–      | 70/201 [00:00<00:00, 297.84it/s, Materializing param=model.layers.7.post_attention_layernorm.weight]Loading weights:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 71/201 [00:00<00:00, 297.84it/s, Materializing param=model.layers.7.self_attn.k_proj.weight]        Loading weights:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 71/201 [00:00<00:00, 297.84it/s, Materializing param=model.layers.7.self_attn.k_proj.weight]Loading weights:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 72/201 [00:00<00:00, 297.84it/s, Materializing param=model.layers.7.self_attn.o_proj.weight]Loading weights:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 72/201 [00:00<00:00, 297.84it/s, Materializing param=model.layers.7.self_attn.o_proj.weight]Loading weights:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 73/201 [00:00<00:00, 297.84it/s, Materializing param=model.layers.7.self_attn.q_proj.weight]Loading weights:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 73/201 [00:00<00:00, 297.84it/s, Materializing param=model.layers.7.self_attn.q_proj.weight]Loading weights:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 74/201 [00:00<00:00, 297.84it/s, Materializing param=model.layers.7.self_attn.v_proj.weight]\u001b[0m\n",
      "\u001b[2K\u001b[31m\u001b[1ALoading weights:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 74/201 [00:00<00:00, 297.84it/s, Materializing param=model.layers.7.self_attn.v_proj.weight]Loading weights:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 75/201 [00:00<00:00, 297.84it/s, Materializing param=model.layers.8.input_layernorm.weight] Loading weights:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 75/201 [00:00<00:00, 297.84it/s, Materializing param=model.layers.8.input_layernorm.weight]Loading weights:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 76/201 [00:00<00:00, 297.84it/s, Materializing param=model.layers.8.mlp.down_proj.weight]  \u001b[0m\n",
      "\u001b[2K\u001b[31m\u001b[1ALoading weights:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 76/201 [00:00<00:00, 297.84it/s, Materializing param=model.layers.8.mlp.down_proj.weight]Loading weights:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 77/201 [00:00<00:00, 297.84it/s, Materializing param=model.layers.8.mlp.gate_proj.weight]\u001b[0m\n",
      "\u001b[2K\u001b[34m-\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0mew app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/preetam\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1ALoading weights:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 77/201 [00:00<00:00, 297.84it/s, Materializing param=model.layers.8.mlp.gate_proj.weight]Loading weights:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 78/201 [00:00<00:00, 297.84it/s, Materializing param=model.layers.8.mlp.up_proj.weight]  \u001b[0m\n",
      "\u001b[2K\u001b[31m\u001b[1ALoading weights:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 78/201 [00:00<00:00, 297.84it/s, Materializing param=model.layers.8.mlp.up_proj.weight]Loading weights:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 79/201 [00:00<00:00, 297.84it/s, Materializing param=model.layers.8.post_attention_layernorm.weight]Loading weights:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 79/201 [00:00<00:00, 297.84it/s, Materializing param=model.layers.8.post_attention_layernorm.weight]Loading weights:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 80/201 [00:00<00:00, 297.84it/s, Materializing param=model.layers.8.self_attn.k_proj.weight]        Loading weights:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 80/201 [00:00<00:00, 297.84it/s, Materializing param=model.layers.8.self_attn.k_proj.weight]Loading weights:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 81/201 [00:00<00:00, 297.84it/s, Materializing param=model.layers.8.self_attn.o_proj.weight]Loading weights:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 81/201 [00:00<00:00, 297.84it/s, Materializing param=model.layers.8.self_attn.o_proj.weight]Loading weights:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 82/201 [00:00<00:00, 297.84it/s, Materializing param=model.layers.8.self_attn.q_proj.weight]Loading weights:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 82/201 [00:00<00:00, 297.84it/s, Materializing param=model.layers.8.self_attn.q_proj.weight]Loading weights:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 83/201 [00:00<00:00, 297.84it/s, Materializing param=model.layers.8.self_attn.v_proj.weight]Loading weights:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 83/201 [00:00<00:00, 297.84it/s, Materializing param=model.layers.8.self_attn.v_proj.weight]Loading weights:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 84/201 [00:00<00:00, 297.84it/s, Materializing param=model.layers.9.input_layernorm.weight] \u001b[0m\n",
      "\u001b[2K\u001b[31m\u001b[1ALoading weights:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 84/201 [00:00<00:00, 297.84it/s, Materializing param=model.layers.9.input_layernorm.weight]Loading weights:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 85/201 [00:00<00:00, 297.84it/s, Materializing param=model.layers.9.mlp.down_proj.weight]  Loading weights:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 85/201 [00:00<00:00, 297.84it/s, Materializing param=model.layers.9.mlp.down_proj.weight]Loading weights:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 86/201 [00:00<00:00, 297.84it/s, Materializing param=model.layers.9.mlp.gate_proj.weight]\u001b[0m\n",
      "\u001b[2K\u001b[31m\u001b[1ALoading weights:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 86/201 [00:00<00:00, 297.84it/s, Materializing param=model.layers.9.mlp.gate_proj.weight]Loading weights:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 87/201 [00:00<00:00, 297.84it/s, Materializing param=model.layers.9.mlp.up_proj.weight]  \u001b[0m\n",
      "\u001b[2K\u001b[31m\u001b[1ALoading weights:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 87/201 [00:00<00:00, 297.84it/s, Materializing param=model.layers.9.mlp.up_proj.weight]Loading weights:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 88/201 [00:00<00:00, 297.84it/s, Materializing param=model.layers.9.post_attention_layernorm.weight]Loading weights:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 88/201 [00:00<00:00, 297.84it/s, Materializing param=model.layers.9.post_attention_layernorm.weight]Loading weights:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 89/201 [00:00<00:00, 297.84it/s, Materializing param=model.layers.9.self_attn.k_proj.weight]        Loading weights:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 89/201 [00:00<00:00, 297.84it/s, Materializing param=model.layers.9.self_attn.k_proj.weight]Loading weights:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 90/201 [00:00<00:00, 297.84it/s, Materializing param=model.layers.9.self_attn.o_proj.weight]Loading weights:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 90/201 [00:00<00:00, 297.84it/s, Materializing param=model.layers.9.self_attn.o_proj.weight]Loading weights:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 91/201 [00:00<00:00, 297.84it/s, Materializing param=model.layers.9.self_attn.q_proj.weight]Loading weights:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 91/201 [00:00<00:00, 297.84it/s, Materializing param=model.layers.9.self_attn.q_proj.weight]Loading weights:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 92/201 [00:00<00:00, 297.84it/s, Materializing param=model.layers.9.self_attn.v_proj.weight]Loading weights:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 92/201 [00:00<00:00, 297.84it/s, Materializing param=model.layers.9.self_attn.v_proj.weight]Loading weights:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 93/201 [00:00<00:00, 297.84it/s, Materializing param=model.layers.10.input_layernorm.weight]Loading weights:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 93/201 [00:00<00:00, 297.84it/s, Materializing param=model.layers.10.input_layernorm.weight]Loading weights:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 94/201 [00:00<00:00, 297.84it/s, Materializing param=model.layers.10.mlp.down_proj.weight]  \u001b[0m\n",
      "\u001b[2K\u001b[31m\u001b[1ALoading weights:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 94/201 [00:00<00:00, 297.84it/s, Materializing param=model.layers.10.mlp.down_proj.weight]Loading weights:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 95/201 [00:00<00:00, 297.84it/s, Materializing param=model.layers.10.mlp.gate_proj.weight]\u001b[0m\n",
      "\u001b[2K\u001b[31m\u001b[1ALoading weights:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 95/201 [00:00<00:00, 297.84it/s, Materializing param=model.layers.10.mlp.gate_proj.weight]Loading weights:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 96/201 [00:00<00:00, 297.84it/s, Materializing param=model.layers.10.mlp.up_proj.weight]  \u001b[0m\n",
      "\u001b[2K\u001b[31m\u001b[1ALoading weights:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 96/201 [00:00<00:00, 297.84it/s, Materializing param=model.layers.10.mlp.up_proj.weight]Loading weights:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 97/201 [00:00<00:00, 297.84it/s, Materializing param=model.layers.10.post_attention_layernorm.weight]Loading weights:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 97/201 [00:00<00:00, 297.84it/s, Materializing param=model.layers.10.post_attention_layernorm.weight]Loading weights:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 98/201 [00:00<00:00, 333.92it/s, Materializing param=model.layers.10.post_attention_layernorm.weight]Loading weights:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 98/201 [00:00<00:00, 333.92it/s, Materializing param=model.layers.10.self_attn.k_proj.weight]        Loading weights:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 98/201 [00:00<00:00, 333.92it/s, Materializing param=model.layers.10.self_attn.k_proj.weight]Loading weights:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 99/201 [00:00<00:00, 333.92it/s, Materializing param=model.layers.10.self_attn.o_proj.weight]Loading weights:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 99/201 [00:00<00:00, 333.92it/s, Materializing param=model.layers.10.self_attn.o_proj.weight]Loading weights:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 100/201 [00:00<00:00, 333.92it/s, Materializing param=model.layers.10.self_attn.q_proj.weight]Loading weights:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 100/201 [00:00<00:00, 333.92it/s, Materializing param=model.layers.10.self_attn.q_proj.weight]Loading weights:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 101/201 [00:00<00:00, 333.92it/s, Materializing param=model.layers.10.self_attn.v_proj.weight]Loading weights:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 101/201 [00:00<00:00, 333.92it/s, Materializing param=model.layers.10.self_attn.v_proj.weight]Loading weights:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 102/201 [00:00<00:00, 333.92it/s, Materializing param=model.layers.11.input_layernorm.weight] \u001b[0m\n",
      "\u001b[2K\u001b[31m\u001b[1ALoading weights:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 102/201 [00:00<00:00, 333.92it/s, Materializing param=model.layers.11.input_layernorm.weight]Loading weights:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 103/201 [00:00<00:00, 333.92it/s, Materializing param=model.layers.11.mlp.down_proj.weight]  \u001b[0m\n",
      "\u001b[2K\u001b[31m\u001b[1ALoading weights:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 103/201 [00:00<00:00, 333.92it/s, Materializing param=model.layers.11.mlp.down_proj.weight]Loading weights:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 104/201 [00:00<00:00, 333.92it/s, Materializing param=model.layers.11.mlp.gate_proj.weight]Loading weights:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 104/201 [00:00<00:00, 333.92it/s, Materializing param=model.layers.11.mlp.gate_proj.weight]Loading weights:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 105/201 [00:00<00:00, 333.92it/s, Materializing param=model.layers.11.mlp.up_proj.weight]  \u001b[0m\n",
      "\u001b[2K\u001b[31m\u001b[1ALoading weights:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 105/201 [00:00<00:00, 333.92it/s, Materializing param=model.layers.11.mlp.up_proj.weight]Loading weights:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 106/201 [00:00<00:00, 333.92it/s, Materializing param=model.layers.11.post_attention_layernorm.weight]Loading weights:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 106/201 [00:00<00:00, 333.92it/s, Materializing param=model.layers.11.post_attention_layernorm.weight]Loading weights:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 107/201 [00:00<00:00, 333.92it/s, Materializing param=model.layers.11.self_attn.k_proj.weight]        Loading weights:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 107/201 [00:00<00:00, 333.92it/s, Materializing param=model.layers.11.self_attn.k_proj.weight]Loading weights:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 108/201 [00:00<00:00, 333.92it/s, Materializing param=model.layers.11.self_attn.o_proj.weight]Loading weights:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 108/201 [00:00<00:00, 333.92it/s, Materializing param=model.layers.11.self_attn.o_proj.weight]Loading weights:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 109/201 [00:00<00:00, 333.92it/s, Materializing param=model.layers.11.self_attn.q_proj.weight]Loading weights:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 109/201 [00:00<00:00, 333.92it/s, Materializing param=model.layers.11.self_attn.q_proj.weight]Loading weights:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 110/201 [00:00<00:00, 333.92it/s, Materializing param=model.layers.11.self_attn.v_proj.weight]Loading weights:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 110/201 [00:00<00:00, 333.92it/s, Materializing param=model.layers.11.self_attn.v_proj.weight]\u001b[0m\n",
      "\u001b[2K\u001b[31m\u001b[1ALoading weights:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 111/201 [00:00<00:00, 333.92it/s, Materializing param=model.layers.12.input_layernorm.weight] Loading weights:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 111/201 [00:00<00:00, 333.92it/s, Materializing param=model.layers.12.input_layernorm.weight]Loading weights:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 112/201 [00:00<00:00, 333.92it/s, Materializing param=model.layers.12.mlp.down_proj.weight]  \u001b[0m\n",
      "\u001b[2K\u001b[31m\u001b[1ALoading weights:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 112/201 [00:00<00:00, 333.92it/s, Materializing param=model.layers.12.mlp.down_proj.weight]Loading weights:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 113/201 [00:00<00:00, 333.92it/s, Materializing param=model.layers.12.mlp.gate_proj.weight]\u001b[0m\n",
      "\u001b[2K\u001b[31m\u001b[1ALoading weights:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 113/201 [00:00<00:00, 333.92it/s, Materializing param=model.layers.12.mlp.gate_proj.weight]Loading weights:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 114/201 [00:00<00:00, 333.92it/s, Materializing param=model.layers.12.mlp.up_proj.weight]  \u001b[0m\n",
      "\u001b[2K\u001b[31m\u001b[1ALoading weights:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 114/201 [00:00<00:00, 333.92it/s, Materializing param=model.layers.12.mlp.up_proj.weight]Loading weights:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 115/201 [00:00<00:00, 333.92it/s, Materializing param=model.layers.12.post_attention_layernorm.weight]Loading weights:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 115/201 [00:00<00:00, 333.92it/s, Materializing param=model.layers.12.post_attention_layernorm.weight]Loading weights:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 116/201 [00:00<00:00, 333.92it/s, Materializing param=model.layers.12.self_attn.k_proj.weight]        Loading weights:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 116/201 [00:00<00:00, 333.92it/s, Materializing param=model.layers.12.self_attn.k_proj.weight]Loading weights:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 117/201 [00:00<00:00, 333.92it/s, Materializing param=model.layers.12.self_attn.o_proj.weight]Loading weights:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 117/201 [00:00<00:00, 333.92it/s, Materializing param=model.layers.12.self_attn.o_proj.weight]Loading weights:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 118/201 [00:00<00:00, 333.92it/s, Materializing param=model.layers.12.self_attn.q_proj.weight]Loading weights:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 118/201 [00:00<00:00, 333.92it/s, Materializing param=model.layers.12.self_attn.q_proj.weight]Loading weights:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 119/201 [00:00<00:00, 333.92it/s, Materializing param=model.layers.12.self_attn.v_proj.weight]Loading weights:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 119/201 [00:00<00:00, 333.92it/s, Materializing param=model.layers.12.self_attn.v_proj.weight]Loading weights:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 120/201 [00:00<00:00, 333.92it/s, Materializing param=model.layers.13.input_layernorm.weight] \u001b[0m\n",
      "\u001b[2K\u001b[31m\u001b[1ALoading weights:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 120/201 [00:00<00:00, 333.92it/s, Materializing param=model.layers.13.input_layernorm.weight]Loading weights:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 121/201 [00:00<00:00, 333.92it/s, Materializing param=model.layers.13.mlp.down_proj.weight]  Loading weights:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 121/201 [00:00<00:00, 333.92it/s, Materializing param=model.layers.13.mlp.down_proj.weight]Loading weights:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 122/201 [00:00<00:00, 333.92it/s, Materializing param=model.layers.13.mlp.gate_proj.weight]\u001b[0m\n",
      "\u001b[2K\u001b[31m\u001b[1ALoading weights:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 122/201 [00:00<00:00, 333.92it/s, Materializing param=model.layers.13.mlp.gate_proj.weight]Loading weights:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 123/201 [00:00<00:00, 333.92it/s, Materializing param=model.layers.13.mlp.up_proj.weight]  \u001b[0m\n",
      "\u001b[2K\u001b[31m\u001b[1ALoading weights:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 123/201 [00:00<00:00, 333.92it/s, Materializing param=model.layers.13.mlp.up_proj.weight]Loading weights:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 124/201 [00:00<00:00, 333.92it/s, Materializing param=model.layers.13.post_attention_layernorm.weight]Loading weights:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 124/201 [00:00<00:00, 333.92it/s, Materializing param=model.layers.13.post_attention_layernorm.weight]Loading weights:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 125/201 [00:00<00:00, 333.92it/s, Materializing param=model.layers.13.self_attn.k_proj.weight]        Loading weights:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 125/201 [00:00<00:00, 333.92it/s, Materializing param=model.layers.13.self_attn.k_proj.weight]Loading weights:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 126/201 [00:00<00:00, 333.92it/s, Materializing param=model.layers.13.self_attn.o_proj.weight]Loading weights:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 126/201 [00:00<00:00, 333.92it/s, Materializing param=model.layers.13.self_attn.o_proj.weight]Loading weights:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 127/201 [00:00<00:00, 333.92it/s, Materializing param=model.layers.13.self_attn.q_proj.weight]Loading weights:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 127/201 [00:00<00:00, 333.92it/s, Materializing param=model.layers.13.self_attn.q_proj.weight]Loading weights:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 128/201 [00:00<00:00, 333.92it/s, Materializing param=model.layers.13.self_attn.v_proj.weight]Loading weights:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 128/201 [00:00<00:00, 333.92it/s, Materializing param=model.layers.13.self_attn.v_proj.weight]Loading weights:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 129/201 [00:00<00:00, 333.92it/s, Materializing param=model.layers.14.input_layernorm.weight] \u001b[0m\n",
      "\u001b[2K\u001b[31m\u001b[1ALoading weights:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 129/201 [00:00<00:00, 333.92it/s, Materializing param=model.layers.14.input_layernorm.weight]Loading weights:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 130/201 [00:00<00:00, 333.92it/s, Materializing param=model.layers.14.mlp.down_proj.weight]  Loading weights:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 130/201 [00:00<00:00, 333.92it/s, Materializing param=model.layers.14.mlp.down_proj.weight]Loading weights:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 131/201 [00:00<00:00, 333.92it/s, Materializing param=model.layers.14.mlp.gate_proj.weight]\u001b[0m\n",
      "\u001b[2K\u001b[31m\u001b[1ALoading weights:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 131/201 [00:00<00:00, 333.92it/s, Materializing param=model.layers.14.mlp.gate_proj.weight]Loading weights:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 132/201 [00:00<00:00, 333.92it/s, Materializing param=model.layers.14.mlp.up_proj.weight]  \u001b[0m\n",
      "\u001b[2K\u001b[31m\u001b[1ALoading weights:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 132/201 [00:00<00:00, 333.92it/s, Materializing param=model.layers.14.mlp.up_proj.weight]Loading weights:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 133/201 [00:00<00:00, 333.92it/s, Materializing param=model.layers.14.post_attention_layernorm.weight]Loading weights:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 133/201 [00:00<00:00, 333.92it/s, Materializing param=model.layers.14.post_attention_layernorm.weight]Loading weights:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 134/201 [00:00<00:00, 333.92it/s, Materializing param=model.layers.14.self_attn.k_proj.weight]        Loading weights:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 134/201 [00:00<00:00, 333.92it/s, Materializing param=model.layers.14.self_attn.k_proj.weight]Loading weights:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 135/201 [00:00<00:00, 333.92it/s, Materializing param=model.layers.14.self_attn.o_proj.weight]Loading weights:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 135/201 [00:00<00:00, 333.92it/s, Materializing param=model.layers.14.self_attn.o_proj.weight]Loading weights:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 136/201 [00:00<00:00, 351.36it/s, Materializing param=model.layers.14.self_attn.o_proj.weight]Loading weights:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 136/201 [00:00<00:00, 351.36it/s, Materializing param=model.layers.14.self_attn.q_proj.weight]Loading weights:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 136/201 [00:00<00:00, 351.36it/s, Materializing param=model.layers.14.self_attn.q_proj.weight]Loading weights:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 137/201 [00:00<00:00, 351.36it/s, Materializing param=model.layers.14.self_attn.v_proj.weight]\u001b[0m\n",
      "\u001b[2K\u001b[31m\u001b[1ALoading weights:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 137/201 [00:00<00:00, 351.36it/s, Materializing param=model.layers.14.self_attn.v_proj.weight]Loading weights:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 138/201 [00:00<00:00, 351.36it/s, Materializing param=model.layers.15.input_layernorm.weight] Loading weights:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 138/201 [00:00<00:00, 351.36it/s, Materializing param=model.layers.15.input_layernorm.weight]Loading weights:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 139/201 [00:00<00:00, 351.36it/s, Materializing param=model.layers.15.mlp.down_proj.weight]  \u001b[0m\n",
      "\u001b[2K\u001b[31m\u001b[1ALoading weights:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 139/201 [00:00<00:00, 351.36it/s, Materializing param=model.layers.15.mlp.down_proj.weight]Loading weights:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 140/201 [00:00<00:00, 351.36it/s, Materializing param=model.layers.15.mlp.gate_proj.weight]Loading weights:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 140/201 [00:00<00:00, 351.36it/s, Materializing param=model.layers.15.mlp.gate_proj.weight]Loading weights:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 141/201 [00:00<00:00, 351.36it/s, Materializing param=model.layers.15.mlp.up_proj.weight]  \u001b[0m\n",
      "\u001b[2K\u001b[31m\u001b[1ALoading weights:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 141/201 [00:00<00:00, 351.36it/s, Materializing param=model.layers.15.mlp.up_proj.weight]Loading weights:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 142/201 [00:00<00:00, 351.36it/s, Materializing param=model.layers.15.post_attention_layernorm.weight]Loading weights:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 142/201 [00:00<00:00, 351.36it/s, Materializing param=model.layers.15.post_attention_layernorm.weight]Loading weights:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 143/201 [00:00<00:00, 351.36it/s, Materializing param=model.layers.15.self_attn.k_proj.weight]        Loading weights:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 143/201 [00:00<00:00, 351.36it/s, Materializing param=model.layers.15.self_attn.k_proj.weight]Loading weights:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 144/201 [00:00<00:00, 351.36it/s, Materializing param=model.layers.15.self_attn.o_proj.weight]Loading weights:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 144/201 [00:00<00:00, 351.36it/s, Materializing param=model.layers.15.self_attn.o_proj.weight]Loading weights:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 145/201 [00:00<00:00, 351.36it/s, Materializing param=model.layers.15.self_attn.q_proj.weight]Loading weights:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 145/201 [00:00<00:00, 351.36it/s, Materializing param=model.layers.15.self_attn.q_proj.weight]Loading weights:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 146/201 [00:00<00:00, 351.36it/s, Materializing param=model.layers.15.self_attn.v_proj.weight]\u001b[0m\n",
      "\u001b[2K\u001b[31m\u001b[1ALoading weights:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 146/201 [00:00<00:00, 351.36it/s, Materializing param=model.layers.15.self_attn.v_proj.weight]Loading weights:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 147/201 [00:00<00:00, 351.36it/s, Materializing param=model.layers.16.input_layernorm.weight] Loading weights:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 147/201 [00:00<00:00, 351.36it/s, Materializing param=model.layers.16.input_layernorm.weight]Loading weights:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 148/201 [00:00<00:00, 351.36it/s, Materializing param=model.layers.16.mlp.down_proj.weight]  \u001b[0m\n",
      "\u001b[2K\u001b[31m\u001b[1ALoading weights:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 148/201 [00:00<00:00, 351.36it/s, Materializing param=model.layers.16.mlp.down_proj.weight]Loading weights:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 149/201 [00:00<00:00, 351.36it/s, Materializing param=model.layers.16.mlp.gate_proj.weight]\u001b[0m\n",
      "\u001b[2K\u001b[31m\u001b[1ALoading weights:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 149/201 [00:00<00:00, 351.36it/s, Materializing param=model.layers.16.mlp.gate_proj.weight]Loading weights:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 150/201 [00:00<00:00, 351.36it/s, Materializing param=model.layers.16.mlp.up_proj.weight]  \u001b[0m\n",
      "\u001b[2K\u001b[31m\u001b[1ALoading weights:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 150/201 [00:00<00:00, 351.36it/s, Materializing param=model.layers.16.mlp.up_proj.weight]Loading weights:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 151/201 [00:00<00:00, 351.36it/s, Materializing param=model.layers.16.post_attention_layernorm.weight]Loading weights:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 151/201 [00:00<00:00, 351.36it/s, Materializing param=model.layers.16.post_attention_layernorm.weight]Loading weights:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 152/201 [00:00<00:00, 351.36it/s, Materializing param=model.layers.16.self_attn.k_proj.weight]        Loading weights:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 152/201 [00:00<00:00, 351.36it/s, Materializing param=model.layers.16.self_attn.k_proj.weight]Loading weights:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 153/201 [00:00<00:00, 351.36it/s, Materializing param=model.layers.16.self_attn.o_proj.weight]Loading weights:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 153/201 [00:00<00:00, 351.36it/s, Materializing param=model.layers.16.self_attn.o_proj.weight]Loading weights:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 154/201 [00:00<00:00, 351.36it/s, Materializing param=model.layers.16.self_attn.q_proj.weight]Loading weights:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 154/201 [00:00<00:00, 351.36it/s, Materializing param=model.layers.16.self_attn.q_proj.weight]Loading weights:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 155/201 [00:00<00:00, 351.36it/s, Materializing param=model.layers.16.self_attn.v_proj.weight]Loading weights:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 155/201 [00:00<00:00, 351.36it/s, Materializing param=model.layers.16.self_attn.v_proj.weight]Loading weights:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 156/201 [00:00<00:00, 351.36it/s, Materializing param=model.layers.17.input_layernorm.weight] Loading weights:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 156/201 [00:00<00:00, 351.36it/s, Materializing param=model.layers.17.input_layernorm.weight]\u001b[0m\n",
      "\u001b[2K\u001b[31m\u001b[1ALoading weights:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 157/201 [00:00<00:00, 351.36it/s, Materializing param=model.layers.17.mlp.down_proj.weight]  \u001b[0m\n",
      "\u001b[2K\u001b[31m\u001b[1ALoading weights:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 157/201 [00:00<00:00, 351.36it/s, Materializing param=model.layers.17.mlp.down_proj.weight]Loading weights:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 158/201 [00:00<00:00, 351.36it/s, Materializing param=model.layers.17.mlp.gate_proj.weight]Loading weights:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 158/201 [00:00<00:00, 351.36it/s, Materializing param=model.layers.17.mlp.gate_proj.weight]\u001b[0m\n",
      "\u001b[2K\u001b[31m\u001b[1ALoading weights:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 159/201 [00:00<00:00, 351.36it/s, Materializing param=model.layers.17.mlp.up_proj.weight]  Loading weights:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 159/201 [00:00<00:00, 351.36it/s, Materializing param=model.layers.17.mlp.up_proj.weight]\u001b[0m\n",
      "\u001b[2K\u001b[31m\u001b[1ALoading weights:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 160/201 [00:00<00:00, 351.36it/s, Materializing param=model.layers.17.post_attention_layernorm.weight]Loading weights:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 160/201 [00:00<00:00, 351.36it/s, Materializing param=model.layers.17.post_attention_layernorm.weight]Loading weights:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 161/201 [00:00<00:00, 351.36it/s, Materializing param=model.layers.17.self_attn.k_proj.weight]        Loading weights:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 161/201 [00:00<00:00, 351.36it/s, Materializing param=model.layers.17.self_attn.k_proj.weight]Loading weights:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 162/201 [00:00<00:00, 351.36it/s, Materializing param=model.layers.17.self_attn.o_proj.weight]Loading weights:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 162/201 [00:00<00:00, 351.36it/s, Materializing param=model.layers.17.self_attn.o_proj.weight]Loading weights:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 163/201 [00:00<00:00, 351.36it/s, Materializing param=model.layers.17.self_attn.q_proj.weight]Loading weights:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 163/201 [00:00<00:00, 351.36it/s, Materializing param=model.layers.17.self_attn.q_proj.weight]Loading weights:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 164/201 [00:00<00:00, 351.36it/s, Materializing param=model.layers.17.self_attn.v_proj.weight]Loading weights:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 164/201 [00:00<00:00, 351.36it/s, Materializing param=model.layers.17.self_attn.v_proj.weight]Loading weights:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 165/201 [00:00<00:00, 351.36it/s, Materializing param=model.layers.18.input_layernorm.weight] Loading weights:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 165/201 [00:00<00:00, 351.36it/s, Materializing param=model.layers.18.input_layernorm.weight]\u001b[0m\n",
      "\u001b[2K\u001b[31m\u001b[1ALoading weights:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 166/201 [00:00<00:00, 351.36it/s, Materializing param=model.layers.18.mlp.down_proj.weight]  Loading weights:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 166/201 [00:00<00:00, 351.36it/s, Materializing param=model.layers.18.mlp.down_proj.weight]Loading weights:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 167/201 [00:00<00:00, 351.36it/s, Materializing param=model.layers.18.mlp.gate_proj.weight]\u001b[0m\n",
      "\u001b[2K\u001b[34m|\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0mew app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/preetam\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1ALoading weights:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 167/201 [00:00<00:00, 351.36it/s, Materializing param=model.layers.18.mlp.gate_proj.weight]Loading weights:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 168/201 [00:00<00:00, 351.36it/s, Materializing param=model.layers.18.mlp.up_proj.weight]  Loading weights:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 168/201 [00:00<00:00, 351.36it/s, Materializing param=model.layers.18.mlp.up_proj.weight]Loading weights:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 169/201 [00:00<00:00, 351.36it/s, Materializing param=model.layers.18.post_attention_layernorm.weight]Loading weights:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 169/201 [00:00<00:00, 351.36it/s, Materializing param=model.layers.18.post_attention_layernorm.weight]Loading weights:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 170/201 [00:00<00:00, 351.36it/s, Materializing param=model.layers.18.self_attn.k_proj.weight]        Loading weights:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 170/201 [00:00<00:00, 351.36it/s, Materializing param=model.layers.18.self_attn.k_proj.weight]Loading weights:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 171/201 [00:00<00:00, 351.36it/s, Materializing param=model.layers.18.self_attn.o_proj.weight]Loading weights:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 171/201 [00:00<00:00, 351.36it/s, Materializing param=model.layers.18.self_attn.o_proj.weight]Loading weights:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 172/201 [00:00<00:00, 350.29it/s, Materializing param=model.layers.18.self_attn.o_proj.weight]Loading weights:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 172/201 [00:00<00:00, 350.29it/s, Materializing param=model.layers.18.self_attn.q_proj.weight]Loading weights:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 172/201 [00:00<00:00, 350.29it/s, Materializing param=model.layers.18.self_attn.q_proj.weight]Loading weights:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 173/201 [00:00<00:00, 350.29it/s, Materializing param=model.layers.18.self_attn.v_proj.weight]Loading weights:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 173/201 [00:00<00:00, 350.29it/s, Materializing param=model.layers.18.self_attn.v_proj.weight]Loading weights:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 174/201 [00:00<00:00, 350.29it/s, Materializing param=model.layers.19.input_layernorm.weight] Loading weights:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 174/201 [00:00<00:00, 350.29it/s, Materializing param=model.layers.19.input_layernorm.weight]Loading weights:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 175/201 [00:00<00:00, 350.29it/s, Materializing param=model.layers.19.mlp.down_proj.weight]  \u001b[0m\n",
      "\u001b[2K\u001b[31m\u001b[1ALoading weights:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 175/201 [00:00<00:00, 350.29it/s, Materializing param=model.layers.19.mlp.down_proj.weight]Loading weights:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 176/201 [00:00<00:00, 350.29it/s, Materializing param=model.layers.19.mlp.gate_proj.weight]\u001b[0m\n",
      "\u001b[2K\u001b[31m\u001b[1ALoading weights:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 176/201 [00:00<00:00, 350.29it/s, Materializing param=model.layers.19.mlp.gate_proj.weight]Loading weights:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 177/201 [00:00<00:00, 350.29it/s, Materializing param=model.layers.19.mlp.up_proj.weight]  \u001b[0m\n",
      "\u001b[2K\u001b[31m\u001b[1ALoading weights:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 177/201 [00:00<00:00, 350.29it/s, Materializing param=model.layers.19.mlp.up_proj.weight]Loading weights:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 178/201 [00:00<00:00, 350.29it/s, Materializing param=model.layers.19.post_attention_layernorm.weight]Loading weights:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 178/201 [00:00<00:00, 350.29it/s, Materializing param=model.layers.19.post_attention_layernorm.weight]Loading weights:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 179/201 [00:00<00:00, 350.29it/s, Materializing param=model.layers.19.self_attn.k_proj.weight]        Loading weights:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 179/201 [00:00<00:00, 350.29it/s, Materializing param=model.layers.19.self_attn.k_proj.weight]Loading weights:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 180/201 [00:00<00:00, 350.29it/s, Materializing param=model.layers.19.self_attn.o_proj.weight]Loading weights:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 180/201 [00:00<00:00, 350.29it/s, Materializing param=model.layers.19.self_attn.o_proj.weight]Loading weights:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 181/201 [00:00<00:00, 350.29it/s, Materializing param=model.layers.19.self_attn.q_proj.weight]Loading weights:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 181/201 [00:00<00:00, 350.29it/s, Materializing param=model.layers.19.self_attn.q_proj.weight]Loading weights:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 182/201 [00:00<00:00, 350.29it/s, Materializing param=model.layers.19.self_attn.v_proj.weight]Loading weights:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 182/201 [00:00<00:00, 350.29it/s, Materializing param=model.layers.19.self_attn.v_proj.weight]Loading weights:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 183/201 [00:00<00:00, 350.29it/s, Materializing param=model.layers.20.input_layernorm.weight] \u001b[0m\n",
      "\u001b[2K\u001b[31m\u001b[1ALoading weights:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 183/201 [00:00<00:00, 350.29it/s, Materializing param=model.layers.20.input_layernorm.weight]Loading weights:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 184/201 [00:00<00:00, 350.29it/s, Materializing param=model.layers.20.mlp.down_proj.weight]  Loading weights:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 184/201 [00:00<00:00, 350.29it/s, Materializing param=model.layers.20.mlp.down_proj.weight]Loading weights:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 185/201 [00:00<00:00, 350.29it/s, Materializing param=model.layers.20.mlp.gate_proj.weight]\u001b[0m\n",
      "\u001b[2K\u001b[31m\u001b[1ALoading weights:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 185/201 [00:00<00:00, 350.29it/s, Materializing param=model.layers.20.mlp.gate_proj.weight]Loading weights:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 186/201 [00:00<00:00, 350.29it/s, Materializing param=model.layers.20.mlp.up_proj.weight]  Loading weights:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 186/201 [00:00<00:00, 350.29it/s, Materializing param=model.layers.20.mlp.up_proj.weight]Loading weights:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 187/201 [00:00<00:00, 350.29it/s, Materializing param=model.layers.20.post_attention_layernorm.weight]Loading weights:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 187/201 [00:00<00:00, 350.29it/s, Materializing param=model.layers.20.post_attention_layernorm.weight]Loading weights:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 188/201 [00:00<00:00, 350.29it/s, Materializing param=model.layers.20.self_attn.k_proj.weight]        Loading weights:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 188/201 [00:00<00:00, 350.29it/s, Materializing param=model.layers.20.self_attn.k_proj.weight]Loading weights:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 189/201 [00:00<00:00, 350.29it/s, Materializing param=model.layers.20.self_attn.o_proj.weight]Loading weights:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 189/201 [00:00<00:00, 350.29it/s, Materializing param=model.layers.20.self_attn.o_proj.weight]Loading weights:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 190/201 [00:00<00:00, 350.29it/s, Materializing param=model.layers.20.self_attn.q_proj.weight]Loading weights:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 190/201 [00:00<00:00, 350.29it/s, Materializing param=model.layers.20.self_attn.q_proj.weight]Loading weights:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 191/201 [00:00<00:00, 350.29it/s, Materializing param=model.layers.20.self_attn.v_proj.weight]Loading weights:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 191/201 [00:00<00:00, 350.29it/s, Materializing param=model.layers.20.self_attn.v_proj.weight]Loading weights:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 192/201 [00:00<00:00, 350.29it/s, Materializing param=model.layers.21.input_layernorm.weight] Loading weights:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 192/201 [00:00<00:00, 350.29it/s, Materializing param=model.layers.21.input_layernorm.weight]Loading weights:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 193/201 [00:00<00:00, 350.29it/s, Materializing param=model.layers.21.mlp.down_proj.weight]  \u001b[0m\n",
      "\u001b[2K\u001b[31m\u001b[1ALoading weights:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 193/201 [00:00<00:00, 350.29it/s, Materializing param=model.layers.21.mlp.down_proj.weight]Loading weights:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 194/201 [00:00<00:00, 350.29it/s, Materializing param=model.layers.21.mlp.gate_proj.weight]\u001b[0m\n",
      "\u001b[2K\u001b[31m\u001b[1ALoading weights:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 194/201 [00:00<00:00, 350.29it/s, Materializing param=model.layers.21.mlp.gate_proj.weight]Loading weights:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 195/201 [00:00<00:00, 350.29it/s, Materializing param=model.layers.21.mlp.up_proj.weight]  Loading weights:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 195/201 [00:00<00:00, 350.29it/s, Materializing param=model.layers.21.mlp.up_proj.weight]Loading weights:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 196/201 [00:00<00:00, 350.29it/s, Materializing param=model.layers.21.post_attention_layernorm.weight]Loading weights:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 196/201 [00:00<00:00, 350.29it/s, Materializing param=model.layers.21.post_attention_layernorm.weight]Loading weights:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 197/201 [00:00<00:00, 350.29it/s, Materializing param=model.layers.21.self_attn.k_proj.weight]        Loading weights:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 197/201 [00:00<00:00, 350.29it/s, Materializing param=model.layers.21.self_attn.k_proj.weight]Loading weights:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 198/201 [00:00<00:00, 350.29it/s, Materializing param=model.layers.21.self_attn.o_proj.weight]Loading weights:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 198/201 [00:00<00:00, 350.29it/s, Materializing param=model.layers.21.self_attn.o_proj.weight]Loading weights:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 199/201 [00:00<00:00, 350.29it/s, Materializing param=model.layers.21.self_attn.q_proj.weight]Loading weights:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 199/201 [00:00<00:00, 350.29it/s, Materializing param=model.layers.21.self_attn.q_proj.weight]Loading weights: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 200/201 [00:00<00:00, 350.29it/s, Materializing param=model.layers.21.self_attn.v_proj.weight]Loading weights: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 200/201 [00:00<00:00, 350.29it/s, Materializing param=model.layers.21.self_attn.v_proj.weight]Loading weights: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 201/201 [00:00<00:00, 350.29it/s, Materializing param=model.norm.weight]                      Loading weights: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 201/201 [00:00<00:00, 350.29it/s, Materializing param=model.norm.weight]Loading weights: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 201/201 [00:00<00:00, 342.31it/s, Materializing param=model.norm.weight]\u001b[0m\n",
      "\u001b[2K\u001b[34m-\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0mew app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/preetam\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m|\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m-\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m|\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m-\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m-\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[4;37mhttps://modal.com/apps/preetam-naik3/main/ap-S7LGHzX5blHrFkLxO63pFN\u001b[0m\n",
      "\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[33mStopping app - local entrypoint completed.\u001b[0m\n",
      "\u001b[32mâœ“\u001b[0m App completed. \u001b[37mView run at \u001b[0m\n",
      "\u001b[4;37mhttps://modal.com/apps/preetam-naik3/main/ap-S7LGHzX5blHrFkLxO63pFN\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<s> Never gonna give you up, never gonna let you down, never'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with modal.enable_output():\n",
    "    with app.run():\n",
    "        result=generate.remote(\"Never gonna give you up, never gonna\")\n",
    "result\n",
    "\n",
    "# Or it you object to being rickrolled, try this: \"Hey Jude, don't make it\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a9a6844-29ec-4264-8e72-362d976b3968",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pricer_ephemeral import app, price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50e6cf99-8959-4ae3-ba02-e325cb7fff94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[32mâœ“\u001b[0m Initialized. \u001b[37mView run at \u001b[0m\n",
      "\u001b[4;37mhttps://modal.com/apps/preetam-naik3/main/ap-dEN6JaQZSmKxa1g7bAlAP8\u001b[0m\n",
      "\u001b[2K\u001b[34m-\u001b[0m Initializing...\n",
      "\u001b[2K\u001b[34m\\\u001b[0m Creating objects...objects...\n",
      "\u001b[90mâ””â”€â”€ \u001b[0m\u001b[34m-\u001b[0m Creating mount \n",
      "\u001b[90m    \u001b[0mc:\\Users\\pritam\\Desktop\\llm_engineering\\week8\\pricer_ephemeral.py: Uploaded\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m Creating objects...\n",
      "\u001b[90mâ””â”€â”€ \u001b[0m\u001b[34m|\u001b[0m Creating mount \n",
      "\u001b[90m    \u001b[0mc:\\Users\\pritam\\Desktop\\llm_engineering\\week8\\pricer_ephemeral.py: Uploaded\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m Creating objects...\n",
      "\u001b[90mâ””â”€â”€ \u001b[0m\u001b[34m/\u001b[0m Creating mount \n",
      "\u001b[90m    \u001b[0mc:\\Users\\pritam\\Desktop\\llm_engineering\\week8\\pricer_ephemeral.py: \n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m Creating objects...\n",
      "\u001b[90mâ””â”€â”€ \u001b[0m\u001b[34m|\u001b[0m Creating mount \n",
      "\u001b[90m    \u001b[0mc:\\Users\\pritam\\Desktop\\llm_engineering\\week8\\pricer_ephemeral.py: \n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m Creating objects...\n",
      "\u001b[90mâ””â”€â”€ \u001b[0m\u001b[34m-\u001b[0m Creating mount \n",
      "\u001b[90m    \u001b[0mc:\\Users\\pritam\\Desktop\\llm_engineering\\week8\\pricer_ephemeral.py: \n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m Creating objects...\n",
      "\u001b[90mâ”œâ”€â”€ \u001b[0mðŸ”¨ Created mount \n",
      "\u001b[90mâ”‚   \u001b[0mc:\\Users\\pritam\\Desktop\\llm_engineering\\week8\\pricer_ephemeral.py\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[34m|\u001b[0m Creating objects...\n",
      "\u001b[90mâ”œâ”€â”€ \u001b[0mðŸ”¨ Created mount \n",
      "\u001b[90mâ”‚   \u001b[0mc:\\Users\\pritam\\Desktop\\llm_engineering\\week8\\pricer_ephemeral.py\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m Creating objects...\n",
      "\u001b[90mâ”œâ”€â”€ \u001b[0mðŸ”¨ Created mount \n",
      "\u001b[90mâ”‚   \u001b[0mc:\\Users\\pritam\\Desktop\\llm_engineering\\week8\\pricer_ephemeral.py\n",
      "\u001b[90mâ””â”€â”€ \u001b[0mðŸ”¨ Created function price.\n",
      "\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[32mâœ“\u001b[0m Created objects.\n",
      "\u001b[90mâ”œâ”€â”€ \u001b[0mðŸ”¨ Created mount \n",
      "\u001b[90mâ”‚   \u001b[0mc:\\Users\\pritam\\Desktop\\llm_engineering\\week8\\pricer_ephemeral.py\n",
      "\u001b[90mâ””â”€â”€ \u001b[0mðŸ”¨ Created function price.\n",
      "\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mWorker assigned...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mWorker assigned...\u001b[0m \u001b[37mView app at \u001b[0mP8\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning...\u001b[0m \u001b[37mView app at \u001b[0ma1g7bAlAP8\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mLoading images (1 containers initializing)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mLoading images (1 containers initializing)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mLoading images (1 containers initializing)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mLoading images (1 containers initializing)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mLoading images (1 containers initializing)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mLoading images (1 containers initializing)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mLoading images (1 containers initializing)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mLoading images (1 containers initializing)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m-\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m-\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m|\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m-\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m|\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m-\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m|\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m-\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m|\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m-\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m|\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m-\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m|\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m-\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m|\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m-\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m|\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m-\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m|\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m-\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m|\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m-\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m|\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m-\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m|\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m-\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m|\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m-\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m|\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m-\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m|\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m-\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m-\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m|\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[31mLoading weights:   0%|          | 0/201 [00:00<?, ?it/s]Loading weights:   0%|          | 1/201 [00:00<00:00, 10538.45it/s, Materializing param=lm_head.weight]\u001b[0m\n",
      "\u001b[2K\u001b[31m\u001b[1ALoading weights:   0%|          | 1/201 [00:00<00:00, 4301.85it/s, Materializing param=lm_head.weight] Loading weights:   1%|          | 2/201 [00:00<00:03, 58.12it/s, Materializing param=model.embed_tokens.weight]Loading weights:   1%|          | 2/201 [00:00<00:03, 57.64it/s, Materializing param=model.embed_tokens.weight]Loading weights:   1%|â–         | 3/201 [00:00<00:02, 85.29it/s, Materializing param=model.layers.0.input_layernorm.weight]Loading weights:   1%|â–         | 3/201 [00:00<00:02, 84.86it/s, Materializing param=model.layers.0.input_layernorm.weight]Loading weights:   2%|â–         | 4/201 [00:00<00:01, 112.19it/s, Materializing param=model.layers.0.mlp.down_proj.weight] \u001b[0m\n",
      "\u001b[2K\u001b[34m|\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0mew app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/preetam\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1ALoading weights:   2%|â–         | 4/201 [00:00<00:01, 111.83it/s, Materializing param=model.layers.0.mlp.down_proj.weight]Loading weights:   2%|â–         | 5/201 [00:00<00:10, 19.00it/s, Materializing param=model.layers.0.mlp.down_proj.weight] Loading weights:   2%|â–         | 5/201 [00:00<00:10, 19.00it/s, Materializing param=model.layers.0.mlp.gate_proj.weight]\u001b[0m\n",
      "\u001b[2K\u001b[31m\u001b[1ALoading weights:   2%|â–         | 5/201 [00:00<00:10, 19.00it/s, Materializing param=model.layers.0.mlp.gate_proj.weight]Loading weights:   3%|â–Ž         | 6/201 [00:00<00:10, 19.00it/s, Materializing param=model.layers.0.mlp.up_proj.weight]  \u001b[0m\n",
      "\u001b[2K\u001b[34m-\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0mew app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/preetam\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1ALoading weights:   3%|â–Ž         | 6/201 [00:00<00:10, 19.00it/s, Materializing param=model.layers.0.mlp.up_proj.weight]Loading weights:   3%|â–Ž         | 7/201 [00:00<00:15, 12.72it/s, Materializing param=model.layers.0.mlp.up_proj.weight]Loading weights:   3%|â–Ž         | 7/201 [00:00<00:15, 12.72it/s, Materializing param=model.layers.0.post_attention_layernorm.weight]Loading weights:   3%|â–Ž         | 7/201 [00:00<00:15, 12.72it/s, Materializing param=model.layers.0.post_attention_layernorm.weight]Loading weights:   4%|â–         | 8/201 [00:00<00:15, 12.72it/s, Materializing param=model.layers.0.self_attn.k_proj.weight]        \u001b[0m\n",
      "\u001b[2K\u001b[31m\u001b[1ALoading weights:   4%|â–         | 8/201 [00:00<00:15, 12.72it/s, Materializing param=model.layers.0.self_attn.k_proj.weight]Loading weights:   4%|â–         | 9/201 [00:00<00:14, 13.04it/s, Materializing param=model.layers.0.self_attn.k_proj.weight]Loading weights:   4%|â–         | 9/201 [00:00<00:14, 13.04it/s, Materializing param=model.layers.0.self_attn.o_proj.weight]\u001b[0m\n",
      "\u001b[2K\u001b[34m|\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0mew app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/preetam\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1ALoading weights:   4%|â–         | 9/201 [00:00<00:14, 13.04it/s, Materializing param=model.layers.0.self_attn.o_proj.weight]Loading weights:   5%|â–         | 10/201 [00:00<00:14, 13.04it/s, Materializing param=model.layers.0.self_attn.q_proj.weight]\u001b[0m\n",
      "\u001b[2K\u001b[31m\u001b[1ALoading weights:   5%|â–         | 10/201 [00:00<00:14, 13.04it/s, Materializing param=model.layers.0.self_attn.q_proj.weight]Loading weights:   5%|â–Œ         | 11/201 [00:00<00:17, 11.01it/s, Materializing param=model.layers.0.self_attn.q_proj.weight]Loading weights:   5%|â–Œ         | 11/201 [00:00<00:17, 11.01it/s, Materializing param=model.layers.0.self_attn.v_proj.weight]Loading weights:   5%|â–Œ         | 11/201 [00:00<00:17, 11.01it/s, Materializing param=model.layers.0.self_attn.v_proj.weight]Loading weights:   6%|â–Œ         | 12/201 [00:00<00:17, 11.01it/s, Materializing param=model.layers.1.input_layernorm.weight] Loading weights:   6%|â–Œ         | 12/201 [00:00<00:17, 11.01it/s, Materializing param=model.layers.1.input_layernorm.weight]Loading weights:   6%|â–‹         | 13/201 [00:00<00:17, 11.01it/s, Materializing param=model.layers.1.mlp.down_proj.weight]  Loading weights:   6%|â–‹         | 13/201 [00:00<00:17, 11.01it/s, Materializing param=model.layers.1.mlp.down_proj.weight]Loading weights:   7%|â–‹         | 14/201 [00:00<00:16, 11.01it/s, Materializing param=model.layers.1.mlp.gate_proj.weight]Loading weights:   7%|â–‹         | 14/201 [00:00<00:16, 11.01it/s, Materializing param=model.layers.1.mlp.gate_proj.weight]Loading weights:   7%|â–‹         | 15/201 [00:00<00:16, 11.01it/s, Materializing param=model.layers.1.mlp.up_proj.weight]  \u001b[0m\n",
      "\u001b[2K\u001b[34m-\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0mew app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/preetam\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1ALoading weights:   7%|â–‹         | 15/201 [00:00<00:16, 11.01it/s, Materializing param=model.layers.1.mlp.up_proj.weight]Loading weights:   8%|â–Š         | 16/201 [00:00<00:16, 11.01it/s, Materializing param=model.layers.1.post_attention_layernorm.weight]Loading weights:   8%|â–Š         | 16/201 [00:00<00:16, 11.01it/s, Materializing param=model.layers.1.post_attention_layernorm.weight]Loading weights:   8%|â–Š         | 17/201 [00:00<00:16, 11.01it/s, Materializing param=model.layers.1.self_attn.k_proj.weight]        Loading weights:   8%|â–Š         | 17/201 [00:00<00:16, 11.01it/s, Materializing param=model.layers.1.self_attn.k_proj.weight]Loading weights:   9%|â–‰         | 18/201 [00:00<00:16, 11.01it/s, Materializing param=model.layers.1.self_attn.o_proj.weight]Loading weights:   9%|â–‰         | 18/201 [00:00<00:16, 11.01it/s, Materializing param=model.layers.1.self_attn.o_proj.weight]Loading weights:   9%|â–‰         | 19/201 [00:00<00:16, 11.01it/s, Materializing param=model.layers.1.self_attn.q_proj.weight]Loading weights:   9%|â–‰         | 19/201 [00:00<00:16, 11.01it/s, Materializing param=model.layers.1.self_attn.q_proj.weight]Loading weights:  10%|â–‰         | 20/201 [00:00<00:16, 11.01it/s, Materializing param=model.layers.1.self_attn.v_proj.weight]Loading weights:  10%|â–‰         | 20/201 [00:00<00:16, 11.01it/s, Materializing param=model.layers.1.self_attn.v_proj.weight]Loading weights:  10%|â–ˆ         | 21/201 [00:00<00:16, 11.01it/s, Materializing param=model.layers.2.input_layernorm.weight] Loading weights:  10%|â–ˆ         | 21/201 [00:00<00:16, 11.01it/s, Materializing param=model.layers.2.input_layernorm.weight]Loading weights:  11%|â–ˆ         | 22/201 [00:00<00:16, 11.01it/s, Materializing param=model.layers.2.mlp.down_proj.weight]  Loading weights:  11%|â–ˆ         | 22/201 [00:00<00:16, 11.01it/s, Materializing param=model.layers.2.mlp.down_proj.weight]Loading weights:  11%|â–ˆâ–        | 23/201 [00:00<00:16, 11.01it/s, Materializing param=model.layers.2.mlp.gate_proj.weight]\u001b[0m\n",
      "\u001b[2K\u001b[31m\u001b[1ALoading weights:  11%|â–ˆâ–        | 23/201 [00:00<00:16, 11.01it/s, Materializing param=model.layers.2.mlp.gate_proj.weight]Loading weights:  12%|â–ˆâ–        | 24/201 [00:00<00:16, 11.01it/s, Materializing param=model.layers.2.mlp.up_proj.weight]  Loading weights:  12%|â–ˆâ–        | 24/201 [00:00<00:16, 11.01it/s, Materializing param=model.layers.2.mlp.up_proj.weight]Loading weights:  12%|â–ˆâ–        | 25/201 [00:00<00:15, 11.01it/s, Materializing param=model.layers.2.post_attention_layernorm.weight]Loading weights:  12%|â–ˆâ–        | 25/201 [00:00<00:15, 11.01it/s, Materializing param=model.layers.2.post_attention_layernorm.weight]Loading weights:  13%|â–ˆâ–Ž        | 26/201 [00:00<00:15, 11.01it/s, Materializing param=model.layers.2.self_attn.k_proj.weight]        Loading weights:  13%|â–ˆâ–Ž        | 26/201 [00:00<00:15, 11.01it/s, Materializing param=model.layers.2.self_attn.k_proj.weight]Loading weights:  13%|â–ˆâ–Ž        | 27/201 [00:00<00:15, 11.01it/s, Materializing param=model.layers.2.self_attn.o_proj.weight]Loading weights:  13%|â–ˆâ–Ž        | 27/201 [00:00<00:15, 11.01it/s, Materializing param=model.layers.2.self_attn.o_proj.weight]Loading weights:  14%|â–ˆâ–        | 28/201 [00:00<00:15, 11.01it/s, Materializing param=model.layers.2.self_attn.q_proj.weight]Loading weights:  14%|â–ˆâ–        | 28/201 [00:00<00:15, 11.01it/s, Materializing param=model.layers.2.self_attn.q_proj.weight]Loading weights:  14%|â–ˆâ–        | 29/201 [00:00<00:15, 11.01it/s, Materializing param=model.layers.2.self_attn.v_proj.weight]Loading weights:  14%|â–ˆâ–        | 29/201 [00:00<00:15, 11.01it/s, Materializing param=model.layers.2.self_attn.v_proj.weight]Loading weights:  15%|â–ˆâ–        | 30/201 [00:00<00:15, 11.01it/s, Materializing param=model.layers.3.input_layernorm.weight] Loading weights:  15%|â–ˆâ–        | 30/201 [00:00<00:15, 11.01it/s, Materializing param=model.layers.3.input_layernorm.weight]Loading weights:  15%|â–ˆâ–Œ        | 31/201 [00:00<00:15, 11.01it/s, Materializing param=model.layers.3.mlp.down_proj.weight]  \u001b[0m\n",
      "\u001b[2K\u001b[31m\u001b[1ALoading weights:  15%|â–ˆâ–Œ        | 31/201 [00:00<00:15, 11.01it/s, Materializing param=model.layers.3.mlp.down_proj.weight]Loading weights:  16%|â–ˆâ–Œ        | 32/201 [00:00<00:15, 11.01it/s, Materializing param=model.layers.3.mlp.gate_proj.weight]Loading weights:  16%|â–ˆâ–Œ        | 32/201 [00:00<00:15, 11.01it/s, Materializing param=model.layers.3.mlp.gate_proj.weight]Loading weights:  16%|â–ˆâ–‹        | 33/201 [00:00<00:15, 11.01it/s, Materializing param=model.layers.3.mlp.up_proj.weight]  Loading weights:  16%|â–ˆâ–‹        | 33/201 [00:00<00:15, 11.01it/s, Materializing param=model.layers.3.mlp.up_proj.weight]Loading weights:  17%|â–ˆâ–‹        | 34/201 [00:00<00:15, 11.01it/s, Materializing param=model.layers.3.post_attention_layernorm.weight]Loading weights:  17%|â–ˆâ–‹        | 34/201 [00:00<00:15, 11.01it/s, Materializing param=model.layers.3.post_attention_layernorm.weight]Loading weights:  17%|â–ˆâ–‹        | 35/201 [00:00<00:15, 11.01it/s, Materializing param=model.layers.3.self_attn.k_proj.weight]        Loading weights:  17%|â–ˆâ–‹        | 35/201 [00:00<00:15, 11.01it/s, Materializing param=model.layers.3.self_attn.k_proj.weight]Loading weights:  18%|â–ˆâ–Š        | 36/201 [00:00<00:14, 11.01it/s, Materializing param=model.layers.3.self_attn.o_proj.weight]Loading weights:  18%|â–ˆâ–Š        | 36/201 [00:00<00:14, 11.01it/s, Materializing param=model.layers.3.self_attn.o_proj.weight]Loading weights:  18%|â–ˆâ–Š        | 37/201 [00:00<00:14, 11.01it/s, Materializing param=model.layers.3.self_attn.q_proj.weight]Loading weights:  18%|â–ˆâ–Š        | 37/201 [00:00<00:14, 11.01it/s, Materializing param=model.layers.3.self_attn.q_proj.weight]Loading weights:  19%|â–ˆâ–‰        | 38/201 [00:00<00:14, 11.01it/s, Materializing param=model.layers.3.self_attn.v_proj.weight]\u001b[0m\n",
      "\u001b[2K\u001b[31m\u001b[1ALoading weights:  19%|â–ˆâ–‰        | 38/201 [00:00<00:14, 11.01it/s, Materializing param=model.layers.3.self_attn.v_proj.weight]Loading weights:  19%|â–ˆâ–‰        | 39/201 [00:00<00:14, 11.01it/s, Materializing param=model.layers.4.input_layernorm.weight] Loading weights:  19%|â–ˆâ–‰        | 39/201 [00:00<00:14, 11.01it/s, Materializing param=model.layers.4.input_layernorm.weight]Loading weights:  20%|â–ˆâ–‰        | 40/201 [00:00<00:14, 11.01it/s, Materializing param=model.layers.4.mlp.down_proj.weight]  Loading weights:  20%|â–ˆâ–‰        | 40/201 [00:00<00:14, 11.01it/s, Materializing param=model.layers.4.mlp.down_proj.weight]Loading weights:  20%|â–ˆâ–ˆ        | 41/201 [00:00<00:14, 11.01it/s, Materializing param=model.layers.4.mlp.gate_proj.weight]Loading weights:  20%|â–ˆâ–ˆ        | 41/201 [00:00<00:14, 11.01it/s, Materializing param=model.layers.4.mlp.gate_proj.weight]Loading weights:  21%|â–ˆâ–ˆ        | 42/201 [00:00<00:14, 11.01it/s, Materializing param=model.layers.4.mlp.up_proj.weight]  Loading weights:  21%|â–ˆâ–ˆ        | 42/201 [00:00<00:14, 11.01it/s, Materializing param=model.layers.4.mlp.up_proj.weight]Loading weights:  21%|â–ˆâ–ˆâ–       | 43/201 [00:00<00:14, 11.01it/s, Materializing param=model.layers.4.post_attention_layernorm.weight]Loading weights:  21%|â–ˆâ–ˆâ–       | 43/201 [00:00<00:14, 11.01it/s, Materializing param=model.layers.4.post_attention_layernorm.weight]Loading weights:  22%|â–ˆâ–ˆâ–       | 44/201 [00:00<00:14, 11.01it/s, Materializing param=model.layers.4.self_attn.k_proj.weight]        Loading weights:  22%|â–ˆâ–ˆâ–       | 44/201 [00:00<00:14, 11.01it/s, Materializing param=model.layers.4.self_attn.k_proj.weight]Loading weights:  22%|â–ˆâ–ˆâ–       | 45/201 [00:00<00:14, 11.01it/s, Materializing param=model.layers.4.self_attn.o_proj.weight]\u001b[0m\n",
      "\u001b[2K\u001b[31m\u001b[1ALoading weights:  22%|â–ˆâ–ˆâ–       | 45/201 [00:00<00:14, 11.01it/s, Materializing param=model.layers.4.self_attn.o_proj.weight]Loading weights:  23%|â–ˆâ–ˆâ–Ž       | 46/201 [00:00<00:14, 11.01it/s, Materializing param=model.layers.4.self_attn.q_proj.weight]Loading weights:  23%|â–ˆâ–ˆâ–Ž       | 46/201 [00:00<00:14, 11.01it/s, Materializing param=model.layers.4.self_attn.q_proj.weight]Loading weights:  23%|â–ˆâ–ˆâ–Ž       | 47/201 [00:00<00:13, 11.01it/s, Materializing param=model.layers.4.self_attn.v_proj.weight]Loading weights:  23%|â–ˆâ–ˆâ–Ž       | 47/201 [00:00<00:13, 11.01it/s, Materializing param=model.layers.4.self_attn.v_proj.weight]Loading weights:  24%|â–ˆâ–ˆâ–       | 48/201 [00:00<00:13, 11.01it/s, Materializing param=model.layers.5.input_layernorm.weight] Loading weights:  24%|â–ˆâ–ˆâ–       | 48/201 [00:00<00:13, 11.01it/s, Materializing param=model.layers.5.input_layernorm.weight]Loading weights:  24%|â–ˆâ–ˆâ–       | 49/201 [00:00<00:13, 11.01it/s, Materializing param=model.layers.5.mlp.down_proj.weight]  Loading weights:  24%|â–ˆâ–ˆâ–       | 49/201 [00:00<00:13, 11.01it/s, Materializing param=model.layers.5.mlp.down_proj.weight]Loading weights:  25%|â–ˆâ–ˆâ–       | 50/201 [00:00<00:13, 11.01it/s, Materializing param=model.layers.5.mlp.gate_proj.weight]Loading weights:  25%|â–ˆâ–ˆâ–       | 50/201 [00:00<00:13, 11.01it/s, Materializing param=model.layers.5.mlp.gate_proj.weight]Loading weights:  25%|â–ˆâ–ˆâ–Œ       | 51/201 [00:00<00:13, 11.01it/s, Materializing param=model.layers.5.mlp.up_proj.weight]  \u001b[0m\n",
      "\u001b[2K\u001b[31m\u001b[1ALoading weights:  25%|â–ˆâ–ˆâ–Œ       | 51/201 [00:00<00:13, 11.01it/s, Materializing param=model.layers.5.mlp.up_proj.weight]Loading weights:  26%|â–ˆâ–ˆâ–Œ       | 52/201 [00:00<00:13, 11.01it/s, Materializing param=model.layers.5.post_attention_layernorm.weight]Loading weights:  26%|â–ˆâ–ˆâ–Œ       | 52/201 [00:00<00:13, 11.01it/s, Materializing param=model.layers.5.post_attention_layernorm.weight]Loading weights:  26%|â–ˆâ–ˆâ–‹       | 53/201 [00:00<00:13, 11.01it/s, Materializing param=model.layers.5.self_attn.k_proj.weight]        Loading weights:  26%|â–ˆâ–ˆâ–‹       | 53/201 [00:00<00:13, 11.01it/s, Materializing param=model.layers.5.self_attn.k_proj.weight]Loading weights:  27%|â–ˆâ–ˆâ–‹       | 54/201 [00:00<00:13, 11.01it/s, Materializing param=model.layers.5.self_attn.o_proj.weight]Loading weights:  27%|â–ˆâ–ˆâ–‹       | 54/201 [00:00<00:13, 11.01it/s, Materializing param=model.layers.5.self_attn.o_proj.weight]Loading weights:  27%|â–ˆâ–ˆâ–‹       | 55/201 [00:00<00:13, 11.01it/s, Materializing param=model.layers.5.self_attn.q_proj.weight]Loading weights:  27%|â–ˆâ–ˆâ–‹       | 55/201 [00:00<00:13, 11.01it/s, Materializing param=model.layers.5.self_attn.q_proj.weight]Loading weights:  28%|â–ˆâ–ˆâ–Š       | 56/201 [00:00<00:13, 11.01it/s, Materializing param=model.layers.5.self_attn.v_proj.weight]Loading weights:  28%|â–ˆâ–ˆâ–Š       | 56/201 [00:00<00:13, 11.01it/s, Materializing param=model.layers.5.self_attn.v_proj.weight]Loading weights:  28%|â–ˆâ–ˆâ–Š       | 57/201 [00:00<00:13, 11.01it/s, Materializing param=model.layers.6.input_layernorm.weight] Loading weights:  28%|â–ˆâ–ˆâ–Š       | 57/201 [00:00<00:13, 11.01it/s, Materializing param=model.layers.6.input_layernorm.weight]Loading weights:  29%|â–ˆâ–ˆâ–‰       | 58/201 [00:00<00:12, 11.01it/s, Materializing param=model.layers.6.mlp.down_proj.weight]  Loading weights:  29%|â–ˆâ–ˆâ–‰       | 58/201 [00:00<00:12, 11.01it/s, Materializing param=model.layers.6.mlp.down_proj.weight]Loading weights:  29%|â–ˆâ–ˆâ–‰       | 59/201 [00:00<00:12, 11.01it/s, Materializing param=model.layers.6.mlp.gate_proj.weight]\u001b[0m\n",
      "\u001b[2K\u001b[31m\u001b[1ALoading weights:  29%|â–ˆâ–ˆâ–‰       | 59/201 [00:00<00:12, 11.01it/s, Materializing param=model.layers.6.mlp.gate_proj.weight]Loading weights:  30%|â–ˆâ–ˆâ–‰       | 60/201 [00:00<00:12, 11.01it/s, Materializing param=model.layers.6.mlp.up_proj.weight]  Loading weights:  30%|â–ˆâ–ˆâ–‰       | 60/201 [00:00<00:12, 11.01it/s, Materializing param=model.layers.6.mlp.up_proj.weight]Loading weights:  30%|â–ˆâ–ˆâ–ˆ       | 61/201 [00:00<00:12, 11.01it/s, Materializing param=model.layers.6.post_attention_layernorm.weight]Loading weights:  30%|â–ˆâ–ˆâ–ˆ       | 61/201 [00:00<00:12, 11.01it/s, Materializing param=model.layers.6.post_attention_layernorm.weight]Loading weights:  31%|â–ˆâ–ˆâ–ˆ       | 62/201 [00:00<00:12, 11.01it/s, Materializing param=model.layers.6.self_attn.k_proj.weight]        Loading weights:  31%|â–ˆâ–ˆâ–ˆ       | 62/201 [00:00<00:12, 11.01it/s, Materializing param=model.layers.6.self_attn.k_proj.weight]Loading weights:  31%|â–ˆâ–ˆâ–ˆâ–      | 63/201 [00:00<00:12, 11.01it/s, Materializing param=model.layers.6.self_attn.o_proj.weight]Loading weights:  31%|â–ˆâ–ˆâ–ˆâ–      | 63/201 [00:00<00:12, 11.01it/s, Materializing param=model.layers.6.self_attn.o_proj.weight]Loading weights:  32%|â–ˆâ–ˆâ–ˆâ–      | 64/201 [00:00<00:12, 11.01it/s, Materializing param=model.layers.6.self_attn.q_proj.weight]Loading weights:  32%|â–ˆâ–ˆâ–ˆâ–      | 64/201 [00:00<00:12, 11.01it/s, Materializing param=model.layers.6.self_attn.q_proj.weight]Loading weights:  32%|â–ˆâ–ˆâ–ˆâ–      | 65/201 [00:00<00:12, 11.01it/s, Materializing param=model.layers.6.self_attn.v_proj.weight]Loading weights:  32%|â–ˆâ–ˆâ–ˆâ–      | 65/201 [00:00<00:12, 11.01it/s, Materializing param=model.layers.6.self_attn.v_proj.weight]Loading weights:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 66/201 [00:00<00:12, 11.01it/s, Materializing param=model.layers.7.input_layernorm.weight] Loading weights:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 66/201 [00:00<00:12, 11.01it/s, Materializing param=model.layers.7.input_layernorm.weight]Loading weights:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 67/201 [00:00<00:12, 11.01it/s, Materializing param=model.layers.7.mlp.down_proj.weight]  \u001b[0m\n",
      "\u001b[2K\u001b[31m\u001b[1ALoading weights:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 67/201 [00:00<00:12, 11.01it/s, Materializing param=model.layers.7.mlp.down_proj.weight]Loading weights:  34%|â–ˆâ–ˆâ–ˆâ–      | 68/201 [00:00<00:12, 11.01it/s, Materializing param=model.layers.7.mlp.gate_proj.weight]Loading weights:  34%|â–ˆâ–ˆâ–ˆâ–      | 68/201 [00:00<00:12, 11.01it/s, Materializing param=model.layers.7.mlp.gate_proj.weight]Loading weights:  34%|â–ˆâ–ˆâ–ˆâ–      | 69/201 [00:00<00:11, 11.01it/s, Materializing param=model.layers.7.mlp.up_proj.weight]  Loading weights:  34%|â–ˆâ–ˆâ–ˆâ–      | 69/201 [00:00<00:11, 11.01it/s, Materializing param=model.layers.7.mlp.up_proj.weight]Loading weights:  35%|â–ˆâ–ˆâ–ˆâ–      | 70/201 [00:00<00:11, 11.01it/s, Materializing param=model.layers.7.post_attention_layernorm.weight]Loading weights:  35%|â–ˆâ–ˆâ–ˆâ–      | 70/201 [00:00<00:11, 11.01it/s, Materializing param=model.layers.7.post_attention_layernorm.weight]Loading weights:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 71/201 [00:00<00:11, 11.01it/s, Materializing param=model.layers.7.self_attn.k_proj.weight]        Loading weights:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 71/201 [00:00<00:11, 11.01it/s, Materializing param=model.layers.7.self_attn.k_proj.weight]Loading weights:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 72/201 [00:00<00:11, 11.01it/s, Materializing param=model.layers.7.self_attn.o_proj.weight]Loading weights:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 72/201 [00:00<00:11, 11.01it/s, Materializing param=model.layers.7.self_attn.o_proj.weight]Loading weights:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 73/201 [00:00<00:11, 11.01it/s, Materializing param=model.layers.7.self_attn.q_proj.weight]\u001b[0m\n",
      "\u001b[2K\u001b[31m\u001b[1ALoading weights:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 73/201 [00:00<00:11, 11.01it/s, Materializing param=model.layers.7.self_attn.q_proj.weight]Loading weights:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 74/201 [00:00<00:11, 11.01it/s, Materializing param=model.layers.7.self_attn.v_proj.weight]Loading weights:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 74/201 [00:00<00:11, 11.01it/s, Materializing param=model.layers.7.self_attn.v_proj.weight]Loading weights:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 75/201 [00:00<00:11, 11.01it/s, Materializing param=model.layers.8.input_layernorm.weight] Loading weights:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 75/201 [00:00<00:11, 11.01it/s, Materializing param=model.layers.8.input_layernorm.weight]Loading weights:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 76/201 [00:00<00:11, 11.01it/s, Materializing param=model.layers.8.mlp.down_proj.weight]  Loading weights:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 76/201 [00:00<00:11, 11.01it/s, Materializing param=model.layers.8.mlp.down_proj.weight]Loading weights:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 77/201 [00:00<00:11, 11.01it/s, Materializing param=model.layers.8.mlp.gate_proj.weight]\u001b[0m\n",
      "\u001b[2K\u001b[34m|\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0mew app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/preetam\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[31m\u001b[1ALoading weights:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 77/201 [00:00<00:11, 11.01it/s, Materializing param=model.layers.8.mlp.gate_proj.weight]Loading weights:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 78/201 [00:00<00:11, 11.01it/s, Materializing param=model.layers.8.mlp.up_proj.weight]  Loading weights:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 78/201 [00:00<00:11, 11.01it/s, Materializing param=model.layers.8.mlp.up_proj.weight]Loading weights:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 79/201 [00:00<00:11, 11.01it/s, Materializing param=model.layers.8.post_attention_layernorm.weight]Loading weights:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 79/201 [00:00<00:11, 11.01it/s, Materializing param=model.layers.8.post_attention_layernorm.weight]Loading weights:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 80/201 [00:00<00:10, 11.01it/s, Materializing param=model.layers.8.self_attn.k_proj.weight]        Loading weights:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 80/201 [00:00<00:10, 11.01it/s, Materializing param=model.layers.8.self_attn.k_proj.weight]Loading weights:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 81/201 [00:00<00:10, 11.01it/s, Materializing param=model.layers.8.self_attn.o_proj.weight]Loading weights:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 81/201 [00:00<00:10, 11.01it/s, Materializing param=model.layers.8.self_attn.o_proj.weight]Loading weights:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 82/201 [00:00<00:10, 11.01it/s, Materializing param=model.layers.8.self_attn.q_proj.weight]Loading weights:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 82/201 [00:00<00:10, 11.01it/s, Materializing param=model.layers.8.self_attn.q_proj.weight]Loading weights:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 83/201 [00:00<00:10, 11.01it/s, Materializing param=model.layers.8.self_attn.v_proj.weight]Loading weights:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 83/201 [00:00<00:10, 11.01it/s, Materializing param=model.layers.8.self_attn.v_proj.weight]Loading weights:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 84/201 [00:00<00:10, 11.01it/s, Materializing param=model.layers.9.input_layernorm.weight] \u001b[0m\n",
      "\u001b[2K\u001b[31m\u001b[1ALoading weights:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 84/201 [00:00<00:10, 11.01it/s, Materializing param=model.layers.9.input_layernorm.weight]Loading weights:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 85/201 [00:00<00:10, 11.01it/s, Materializing param=model.layers.9.mlp.down_proj.weight]  Loading weights:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 85/201 [00:00<00:10, 11.01it/s, Materializing param=model.layers.9.mlp.down_proj.weight]Loading weights:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 86/201 [00:00<00:10, 11.01it/s, Materializing param=model.layers.9.mlp.gate_proj.weight]Loading weights:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 86/201 [00:00<00:10, 11.01it/s, Materializing param=model.layers.9.mlp.gate_proj.weight]Loading weights:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 87/201 [00:00<00:10, 11.01it/s, Materializing param=model.layers.9.mlp.up_proj.weight]  Loading weights:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 87/201 [00:00<00:10, 11.01it/s, Materializing param=model.layers.9.mlp.up_proj.weight]Loading weights:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 88/201 [00:00<00:10, 11.01it/s, Materializing param=model.layers.9.post_attention_layernorm.weight]Loading weights:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 88/201 [00:00<00:10, 11.01it/s, Materializing param=model.layers.9.post_attention_layernorm.weight]Loading weights:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 89/201 [00:00<00:10, 11.01it/s, Materializing param=model.layers.9.self_attn.k_proj.weight]        Loading weights:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 89/201 [00:00<00:10, 11.01it/s, Materializing param=model.layers.9.self_attn.k_proj.weight]Loading weights:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 90/201 [00:00<00:10, 11.01it/s, Materializing param=model.layers.9.self_attn.o_proj.weight]Loading weights:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 90/201 [00:00<00:10, 11.01it/s, Materializing param=model.layers.9.self_attn.o_proj.weight]Loading weights:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 91/201 [00:00<00:09, 11.01it/s, Materializing param=model.layers.9.self_attn.q_proj.weight]\u001b[0m\n",
      "\u001b[2K\u001b[31m\u001b[1ALoading weights:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 91/201 [00:00<00:09, 11.01it/s, Materializing param=model.layers.9.self_attn.q_proj.weight]Loading weights:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 92/201 [00:00<00:09, 11.01it/s, Materializing param=model.layers.9.self_attn.v_proj.weight]Loading weights:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 92/201 [00:00<00:09, 11.01it/s, Materializing param=model.layers.9.self_attn.v_proj.weight]Loading weights:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 93/201 [00:00<00:09, 11.01it/s, Materializing param=model.layers.10.input_layernorm.weight]Loading weights:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 93/201 [00:00<00:09, 11.01it/s, Materializing param=model.layers.10.input_layernorm.weight]Loading weights:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 94/201 [00:00<00:09, 11.01it/s, Materializing param=model.layers.10.mlp.down_proj.weight]  Loading weights:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 94/201 [00:00<00:09, 11.01it/s, Materializing param=model.layers.10.mlp.down_proj.weight]Loading weights:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 95/201 [00:00<00:09, 11.01it/s, Materializing param=model.layers.10.mlp.gate_proj.weight]Loading weights:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 95/201 [00:00<00:09, 11.01it/s, Materializing param=model.layers.10.mlp.gate_proj.weight]Loading weights:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 96/201 [00:00<00:09, 11.01it/s, Materializing param=model.layers.10.mlp.up_proj.weight]  Loading weights:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 96/201 [00:00<00:09, 11.01it/s, Materializing param=model.layers.10.mlp.up_proj.weight]Loading weights:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 97/201 [00:00<00:09, 11.01it/s, Materializing param=model.layers.10.post_attention_layernorm.weight]Loading weights:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 97/201 [00:00<00:09, 11.01it/s, Materializing param=model.layers.10.post_attention_layernorm.weight]Loading weights:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 98/201 [00:00<00:09, 11.01it/s, Materializing param=model.layers.10.self_attn.k_proj.weight]        \u001b[0m\n",
      "\u001b[2K\u001b[31m\u001b[1ALoading weights:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 98/201 [00:00<00:09, 11.01it/s, Materializing param=model.layers.10.self_attn.k_proj.weight]Loading weights:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 99/201 [00:00<00:09, 11.01it/s, Materializing param=model.layers.10.self_attn.o_proj.weight]Loading weights:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 99/201 [00:00<00:09, 11.01it/s, Materializing param=model.layers.10.self_attn.o_proj.weight]Loading weights:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 100/201 [00:00<00:09, 11.01it/s, Materializing param=model.layers.10.self_attn.q_proj.weight]Loading weights:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 100/201 [00:00<00:09, 11.01it/s, Materializing param=model.layers.10.self_attn.q_proj.weight]Loading weights:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 101/201 [00:00<00:09, 11.01it/s, Materializing param=model.layers.10.self_attn.v_proj.weight]Loading weights:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 101/201 [00:00<00:09, 11.01it/s, Materializing param=model.layers.10.self_attn.v_proj.weight]Loading weights:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 102/201 [00:00<00:08, 11.01it/s, Materializing param=model.layers.11.input_layernorm.weight] Loading weights:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 102/201 [00:00<00:08, 11.01it/s, Materializing param=model.layers.11.input_layernorm.weight]Loading weights:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 103/201 [00:00<00:08, 11.01it/s, Materializing param=model.layers.11.mlp.down_proj.weight]  Loading weights:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 103/201 [00:00<00:08, 11.01it/s, Materializing param=model.layers.11.mlp.down_proj.weight]Loading weights:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 104/201 [00:00<00:08, 11.01it/s, Materializing param=model.layers.11.mlp.gate_proj.weight]Loading weights:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 104/201 [00:00<00:08, 11.01it/s, Materializing param=model.layers.11.mlp.gate_proj.weight]Loading weights:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 105/201 [00:00<00:08, 11.01it/s, Materializing param=model.layers.11.mlp.up_proj.weight]  \u001b[0m\n",
      "\u001b[2K\u001b[31m\u001b[1ALoading weights:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 105/201 [00:00<00:08, 11.01it/s, Materializing param=model.layers.11.mlp.up_proj.weight]Loading weights:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 106/201 [00:00<00:08, 11.01it/s, Materializing param=model.layers.11.post_attention_layernorm.weight]Loading weights:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 106/201 [00:00<00:08, 11.01it/s, Materializing param=model.layers.11.post_attention_layernorm.weight]Loading weights:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 107/201 [00:00<00:08, 11.01it/s, Materializing param=model.layers.11.self_attn.k_proj.weight]        Loading weights:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 107/201 [00:00<00:08, 11.01it/s, Materializing param=model.layers.11.self_attn.k_proj.weight]Loading weights:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 108/201 [00:00<00:08, 11.01it/s, Materializing param=model.layers.11.self_attn.o_proj.weight]Loading weights:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 108/201 [00:00<00:08, 11.01it/s, Materializing param=model.layers.11.self_attn.o_proj.weight]Loading weights:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 109/201 [00:00<00:08, 11.01it/s, Materializing param=model.layers.11.self_attn.q_proj.weight]Loading weights:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 109/201 [00:00<00:08, 11.01it/s, Materializing param=model.layers.11.self_attn.q_proj.weight]Loading weights:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 110/201 [00:00<00:08, 11.01it/s, Materializing param=model.layers.11.self_attn.v_proj.weight]Loading weights:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 110/201 [00:00<00:08, 11.01it/s, Materializing param=model.layers.11.self_attn.v_proj.weight]Loading weights:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 111/201 [00:00<00:08, 11.01it/s, Materializing param=model.layers.12.input_layernorm.weight] Loading weights:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 111/201 [00:00<00:08, 11.01it/s, Materializing param=model.layers.12.input_layernorm.weight]Loading weights:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 112/201 [00:00<00:08, 11.01it/s, Materializing param=model.layers.12.mlp.down_proj.weight]  Loading weights:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 112/201 [00:00<00:08, 11.01it/s, Materializing param=model.layers.12.mlp.down_proj.weight]Loading weights:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 113/201 [00:00<00:07, 11.01it/s, Materializing param=model.layers.12.mlp.gate_proj.weight]\u001b[0m\n",
      "\u001b[2K\u001b[31m\u001b[1ALoading weights:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 113/201 [00:00<00:07, 11.01it/s, Materializing param=model.layers.12.mlp.gate_proj.weight]Loading weights:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 114/201 [00:00<00:07, 11.01it/s, Materializing param=model.layers.12.mlp.up_proj.weight]  Loading weights:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 114/201 [00:00<00:07, 11.01it/s, Materializing param=model.layers.12.mlp.up_proj.weight]Loading weights:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 115/201 [00:00<00:07, 11.01it/s, Materializing param=model.layers.12.post_attention_layernorm.weight]Loading weights:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 115/201 [00:00<00:07, 11.01it/s, Materializing param=model.layers.12.post_attention_layernorm.weight]Loading weights:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 116/201 [00:00<00:07, 11.01it/s, Materializing param=model.layers.12.self_attn.k_proj.weight]        Loading weights:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 116/201 [00:00<00:07, 11.01it/s, Materializing param=model.layers.12.self_attn.k_proj.weight]Loading weights:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 117/201 [00:00<00:07, 11.01it/s, Materializing param=model.layers.12.self_attn.o_proj.weight]Loading weights:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 117/201 [00:00<00:07, 11.01it/s, Materializing param=model.layers.12.self_attn.o_proj.weight]Loading weights:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 118/201 [00:00<00:00, 227.93it/s, Materializing param=model.layers.12.self_attn.o_proj.weight]Loading weights:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 118/201 [00:00<00:00, 227.93it/s, Materializing param=model.layers.12.self_attn.q_proj.weight]Loading weights:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 118/201 [00:00<00:00, 227.93it/s, Materializing param=model.layers.12.self_attn.q_proj.weight]Loading weights:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 119/201 [00:00<00:00, 227.93it/s, Materializing param=model.layers.12.self_attn.v_proj.weight]Loading weights:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 119/201 [00:00<00:00, 227.93it/s, Materializing param=model.layers.12.self_attn.v_proj.weight]Loading weights:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 120/201 [00:00<00:00, 227.93it/s, Materializing param=model.layers.13.input_layernorm.weight] Loading weights:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 120/201 [00:00<00:00, 227.93it/s, Materializing param=model.layers.13.input_layernorm.weight]Loading weights:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 121/201 [00:00<00:00, 227.93it/s, Materializing param=model.layers.13.mlp.down_proj.weight]  \u001b[0m\n",
      "\u001b[2K\u001b[31m\u001b[1ALoading weights:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 121/201 [00:00<00:00, 227.93it/s, Materializing param=model.layers.13.mlp.down_proj.weight]Loading weights:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 122/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.13.mlp.gate_proj.weight]Loading weights:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 122/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.13.mlp.gate_proj.weight]Loading weights:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 123/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.13.mlp.up_proj.weight]  Loading weights:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 123/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.13.mlp.up_proj.weight]Loading weights:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 124/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.13.post_attention_layernorm.weight]Loading weights:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 124/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.13.post_attention_layernorm.weight]Loading weights:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 125/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.13.self_attn.k_proj.weight]        Loading weights:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 125/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.13.self_attn.k_proj.weight]Loading weights:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 126/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.13.self_attn.o_proj.weight]Loading weights:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 126/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.13.self_attn.o_proj.weight]Loading weights:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 127/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.13.self_attn.q_proj.weight]Loading weights:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 127/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.13.self_attn.q_proj.weight]Loading weights:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 128/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.13.self_attn.v_proj.weight]\u001b[0m\n",
      "\u001b[2K\u001b[31m\u001b[1ALoading weights:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 128/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.13.self_attn.v_proj.weight]Loading weights:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 129/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.14.input_layernorm.weight] Loading weights:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 129/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.14.input_layernorm.weight]Loading weights:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 130/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.14.mlp.down_proj.weight]  Loading weights:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 130/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.14.mlp.down_proj.weight]Loading weights:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 131/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.14.mlp.gate_proj.weight]Loading weights:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 131/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.14.mlp.gate_proj.weight]Loading weights:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 132/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.14.mlp.up_proj.weight]  Loading weights:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 132/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.14.mlp.up_proj.weight]Loading weights:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 133/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.14.post_attention_layernorm.weight]Loading weights:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 133/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.14.post_attention_layernorm.weight]Loading weights:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 134/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.14.self_attn.k_proj.weight]        \u001b[0m\n",
      "\u001b[2K\u001b[31m\u001b[1ALoading weights:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 134/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.14.self_attn.k_proj.weight]Loading weights:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 135/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.14.self_attn.o_proj.weight]Loading weights:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 135/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.14.self_attn.o_proj.weight]Loading weights:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 136/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.14.self_attn.q_proj.weight]Loading weights:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 136/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.14.self_attn.q_proj.weight]Loading weights:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 137/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.14.self_attn.v_proj.weight]Loading weights:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 137/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.14.self_attn.v_proj.weight]Loading weights:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 138/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.15.input_layernorm.weight] Loading weights:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 138/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.15.input_layernorm.weight]Loading weights:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 139/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.15.mlp.down_proj.weight]  \u001b[0m\n",
      "\u001b[2K\u001b[31m\u001b[1ALoading weights:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 139/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.15.mlp.down_proj.weight]Loading weights:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 140/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.15.mlp.gate_proj.weight]Loading weights:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 140/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.15.mlp.gate_proj.weight]Loading weights:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 141/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.15.mlp.up_proj.weight]  Loading weights:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 141/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.15.mlp.up_proj.weight]Loading weights:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 142/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.15.post_attention_layernorm.weight]Loading weights:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 142/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.15.post_attention_layernorm.weight]Loading weights:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 143/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.15.self_attn.k_proj.weight]        Loading weights:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 143/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.15.self_attn.k_proj.weight]Loading weights:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 144/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.15.self_attn.o_proj.weight]Loading weights:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 144/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.15.self_attn.o_proj.weight]Loading weights:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 145/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.15.self_attn.q_proj.weight]Loading weights:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 145/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.15.self_attn.q_proj.weight]Loading weights:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 146/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.15.self_attn.v_proj.weight]\u001b[0m\n",
      "\u001b[2K\u001b[31m\u001b[1ALoading weights:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 146/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.15.self_attn.v_proj.weight]Loading weights:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 147/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.16.input_layernorm.weight] Loading weights:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 147/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.16.input_layernorm.weight]Loading weights:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 148/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.16.mlp.down_proj.weight]  Loading weights:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 148/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.16.mlp.down_proj.weight]Loading weights:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 149/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.16.mlp.gate_proj.weight]Loading weights:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 149/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.16.mlp.gate_proj.weight]Loading weights:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 150/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.16.mlp.up_proj.weight]  Loading weights:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 150/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.16.mlp.up_proj.weight]Loading weights:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 151/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.16.post_attention_layernorm.weight]Loading weights:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 151/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.16.post_attention_layernorm.weight]Loading weights:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 152/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.16.self_attn.k_proj.weight]        \u001b[0m\n",
      "\u001b[2K\u001b[31m\u001b[1ALoading weights:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 152/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.16.self_attn.k_proj.weight]Loading weights:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 153/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.16.self_attn.o_proj.weight]Loading weights:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 153/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.16.self_attn.o_proj.weight]Loading weights:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 154/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.16.self_attn.q_proj.weight]Loading weights:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 154/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.16.self_attn.q_proj.weight]Loading weights:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 155/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.16.self_attn.v_proj.weight]Loading weights:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 155/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.16.self_attn.v_proj.weight]Loading weights:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 156/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.17.input_layernorm.weight] Loading weights:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 156/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.17.input_layernorm.weight]Loading weights:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 157/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.17.mlp.down_proj.weight]  Loading weights:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 157/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.17.mlp.down_proj.weight]Loading weights:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 158/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.17.mlp.gate_proj.weight]Loading weights:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 158/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.17.mlp.gate_proj.weight]Loading weights:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 159/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.17.mlp.up_proj.weight]  \u001b[0m\n",
      "\u001b[2K\u001b[31m\u001b[1ALoading weights:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 159/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.17.mlp.up_proj.weight]Loading weights:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 160/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.17.post_attention_layernorm.weight]Loading weights:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 160/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.17.post_attention_layernorm.weight]Loading weights:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 161/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.17.self_attn.k_proj.weight]        Loading weights:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 161/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.17.self_attn.k_proj.weight]Loading weights:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 162/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.17.self_attn.o_proj.weight]Loading weights:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 162/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.17.self_attn.o_proj.weight]Loading weights:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 163/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.17.self_attn.q_proj.weight]Loading weights:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 163/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.17.self_attn.q_proj.weight]Loading weights:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 164/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.17.self_attn.v_proj.weight]Loading weights:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 164/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.17.self_attn.v_proj.weight]Loading weights:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 165/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.18.input_layernorm.weight] Loading weights:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 165/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.18.input_layernorm.weight]Loading weights:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 166/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.18.mlp.down_proj.weight]  Loading weights:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 166/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.18.mlp.down_proj.weight]Loading weights:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 167/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.18.mlp.gate_proj.weight]\u001b[0m\n",
      "\u001b[2K\u001b[31m\u001b[1ALoading weights:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 167/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.18.mlp.gate_proj.weight]Loading weights:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 168/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.18.mlp.up_proj.weight]  Loading weights:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 168/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.18.mlp.up_proj.weight]Loading weights:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 169/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.18.post_attention_layernorm.weight]Loading weights:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 169/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.18.post_attention_layernorm.weight]Loading weights:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 170/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.18.self_attn.k_proj.weight]        Loading weights:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 170/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.18.self_attn.k_proj.weight]Loading weights:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 171/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.18.self_attn.o_proj.weight]Loading weights:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 171/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.18.self_attn.o_proj.weight]Loading weights:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 172/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.18.self_attn.q_proj.weight]Loading weights:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 172/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.18.self_attn.q_proj.weight]Loading weights:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 173/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.18.self_attn.v_proj.weight]Loading weights:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 173/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.18.self_attn.v_proj.weight]Loading weights:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 174/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.19.input_layernorm.weight] Loading weights:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 174/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.19.input_layernorm.weight]Loading weights:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 175/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.19.mlp.down_proj.weight]  \u001b[0m\n",
      "\u001b[2K\u001b[31m\u001b[1ALoading weights:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 175/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.19.mlp.down_proj.weight]Loading weights:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 176/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.19.mlp.gate_proj.weight]Loading weights:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 176/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.19.mlp.gate_proj.weight]Loading weights:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 177/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.19.mlp.up_proj.weight]  Loading weights:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 177/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.19.mlp.up_proj.weight]Loading weights:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 178/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.19.post_attention_layernorm.weight]Loading weights:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 178/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.19.post_attention_layernorm.weight]Loading weights:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 179/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.19.self_attn.k_proj.weight]        Loading weights:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 179/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.19.self_attn.k_proj.weight]Loading weights:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 180/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.19.self_attn.o_proj.weight]Loading weights:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 180/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.19.self_attn.o_proj.weight]Loading weights:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 181/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.19.self_attn.q_proj.weight]Loading weights:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 181/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.19.self_attn.q_proj.weight]Loading weights:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 182/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.19.self_attn.v_proj.weight]\u001b[0m\n",
      "\u001b[2K\u001b[31m\u001b[1ALoading weights:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 182/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.19.self_attn.v_proj.weight]Loading weights:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 183/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.20.input_layernorm.weight] Loading weights:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 183/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.20.input_layernorm.weight]Loading weights:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 184/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.20.mlp.down_proj.weight]  Loading weights:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 184/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.20.mlp.down_proj.weight]Loading weights:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 185/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.20.mlp.gate_proj.weight]Loading weights:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 185/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.20.mlp.gate_proj.weight]Loading weights:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 186/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.20.mlp.up_proj.weight]  Loading weights:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 186/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.20.mlp.up_proj.weight]Loading weights:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 187/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.20.post_attention_layernorm.weight]Loading weights:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 187/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.20.post_attention_layernorm.weight]Loading weights:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 188/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.20.self_attn.k_proj.weight]        Loading weights:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 188/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.20.self_attn.k_proj.weight]\u001b[0m\n",
      "\u001b[2K\u001b[31m\u001b[1ALoading weights:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 189/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.20.self_attn.o_proj.weight]Loading weights:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 189/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.20.self_attn.o_proj.weight]Loading weights:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 190/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.20.self_attn.q_proj.weight]Loading weights:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 190/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.20.self_attn.q_proj.weight]Loading weights:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 191/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.20.self_attn.v_proj.weight]Loading weights:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 191/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.20.self_attn.v_proj.weight]Loading weights:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 192/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.21.input_layernorm.weight] Loading weights:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 192/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.21.input_layernorm.weight]Loading weights:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 193/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.21.mlp.down_proj.weight]  Loading weights:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 193/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.21.mlp.down_proj.weight]Loading weights:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 194/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.21.mlp.gate_proj.weight]\u001b[0m\n",
      "\u001b[2K\u001b[31m\u001b[1ALoading weights:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 194/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.21.mlp.gate_proj.weight]Loading weights:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 195/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.21.mlp.up_proj.weight]  Loading weights:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 195/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.21.mlp.up_proj.weight]Loading weights:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 196/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.21.post_attention_layernorm.weight]Loading weights:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 196/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.21.post_attention_layernorm.weight]Loading weights:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 197/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.21.self_attn.k_proj.weight]        Loading weights:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 197/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.21.self_attn.k_proj.weight]Loading weights:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 198/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.21.self_attn.o_proj.weight]Loading weights:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 198/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.21.self_attn.o_proj.weight]Loading weights:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 199/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.21.self_attn.q_proj.weight]Loading weights:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 199/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.21.self_attn.q_proj.weight]Loading weights: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 200/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.21.self_attn.v_proj.weight]Loading weights: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 200/201 [00:01<00:00, 227.93it/s, Materializing param=model.layers.21.self_attn.v_proj.weight]Loading weights: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 201/201 [00:01<00:00, 227.93it/s, Materializing param=model.norm.weight]                      Loading weights: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 201/201 [00:01<00:00, 227.93it/s, Materializing param=model.norm.weight]Loading weights: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 201/201 [00:01<00:00, 186.95it/s, Materializing param=model.norm.weight]\u001b[0m\n",
      "\u001b[2K\u001b[34m-\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0mew app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/preetam\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m|\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m-\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m|\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m-\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m|\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m-\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m|\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m-\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m|\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m-\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m|\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m-\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m|\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m-\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m|\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m-\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m|\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m-\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m|\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m-\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m|\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m-\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m|\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m-\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m|\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m-\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m|\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m|\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[31mTraceback (most recent call last):\u001b[0m6JaQZSmKxa1g7bAlAP8\u001b[0m\n",
      "\u001b[31m  File \"/pkg/modal/_runtime/container_io_manager.py\", line 947, in handle_input_exception\u001b[0m\n",
      "\u001b[31m    yield\u001b[0m\n",
      "\u001b[31m  File \"/pkg/modal/_container_entrypoint.py\", line 172, in run_input_sync\u001b[0m\n",
      "\u001b[31m    values = io_context.call_function_sync()\u001b[0m\n",
      "\u001b[31m  File \"/pkg/modal/_runtime/container_io_manager.py\", line 225, in call_function_sync\u001b[0m\n",
      "\u001b[31m    expected_value_or_values = self.finalized_function.callable(*args, **kwargs)\u001b[0m\n",
      "\u001b[31m  File \"/root/pricer_ephemeral.py\", line 54, in price\u001b[0m\n",
      "\u001b[31m    fine_tuned_model = PeftModel.from_pretrained(base_model, FINETUNED_MODEL, revision=REVISION)\u001b[0m\n",
      "\u001b[31m  File \"/usr/local/lib/python3.10/site-packages/peft/peft_model.py\", line 568, in from_pretrained\u001b[0m\n",
      "\u001b[31m    load_result = model.load_adapter(\u001b[0m\n",
      "\u001b[31m  File \"/usr/local/lib/python3.10/site-packages/peft/peft_model.py\", line 1368, in load_adapter\u001b[0m\n",
      "\u001b[31m    load_result = set_peft_model_state_dict(\u001b[0m\n",
      "\u001b[31m  File \"/usr/local/lib/python3.10/site-packages/peft/utils/save_and_load.py\", line 565, in set_peft_model_state_dict\u001b[0m\n",
      "\u001b[31m    load_result = model.load_state_dict(peft_model_state_dict, strict=False)\u001b[0m\n",
      "\u001b[31m  File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 2635, in load_state_dict\u001b[0m\n",
      "\u001b[31m    raise RuntimeError(\u001b[0m\n",
      "\u001b[31mRuntimeError: Error(s) in loading state_dict for PeftModelForCausalLM:\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 8192]) from checkpoint, the shape in current model is torch.Size([256, 5632]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 8192]) from checkpoint, the shape in current model is torch.Size([256, 5632]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\u001b[0m\n",
      "\u001b[2K\u001b[31m        size mismatch for base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 8192]) from checkpoint, the shape in current model is torch.Size([256, 5632]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 8192]) from checkpoint, the shape in current model is torch.Size([256, 5632]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 8192]) from checkpoint, the shape in current model is torch.Size([256, 5632]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[2K\u001b[34m\\\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0mew app at \u001b[0m\u001b[4;37mhttps://modal.com/apps/preetam\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m/\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[34m-\u001b[0m \u001b[34mRunning (1/1 containers active)...\u001b[0m \u001b[37mView app at \u001b[0m\n",
      "\u001b[4;37mhttps://modal.com/apps/preetam-naik3/main/ap-dEN6JaQZSmKxa1g7bAlAP8\u001b[0m\n",
      "\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[31m        size mismatch for base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 8192]) from checkpoint, the shape in current model is torch.Size([256, 5632]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 8192]) from checkpoint, the shape in current model is torch.Size([256, 5632]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 8192]) from checkpoint, the shape in current model is torch.Size([256, 5632]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 8192]) from checkpoint, the shape in current model is torch.Size([256, 5632]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 8192]) from checkpoint, the shape in current model is torch.Size([256, 5632]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 8192]) from checkpoint, the shape in current model is torch.Size([256, 5632]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 8192]) from checkpoint, the shape in current model is torch.Size([256, 5632]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 8192]) from checkpoint, the shape in current model is torch.Size([256, 5632]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 8192]) from checkpoint, the shape in current model is torch.Size([256, 5632]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 8192]) from checkpoint, the shape in current model is torch.Size([256, 5632]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 8192]) from checkpoint, the shape in current model is torch.Size([256, 5632]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.16.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.16.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.16.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.16.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 8192]) from checkpoint, the shape in current model is torch.Size([256, 5632]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.17.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.17.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.17.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.17.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 8192]) from checkpoint, the shape in current model is torch.Size([256, 5632]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.18.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.18.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.18.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.18.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 8192]) from checkpoint, the shape in current model is torch.Size([256, 5632]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.19.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.19.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.19.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.19.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 8192]) from checkpoint, the shape in current model is torch.Size([256, 5632]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\u001b[0m\n",
      "\u001b[31m        size mismatch for base_model.model.model.layers.20.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, t\u001b[0m\n",
      "\u001b[33mStopping app - uncaught exception raised in remote container: RuntimeError('Error(s) in loading state_dict for PeftModelForCausalLM:\\n\\tsize mismatch for base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\\n\\tsize mismatch for base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\\n\\tsize mismatch for base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\\n\\tsize mismatch for base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\\n\\tsize mismatch for base_model.model.model.layers.0.self_attn.v_p.\u001b[0m\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for PeftModelForCausalLM:\n\tsize mismatch for base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\n\tsize mismatch for base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\n\tsize mismatch for base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 8192]) from checkpoint, the shape in current model is torch.Size([256, 5632]).\n\tsize mismatch for base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\n\tsize mismatch for base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\n\tsize mismatch for base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 8192]) from checkpoint, the shape in current model is torch.Size([256, 5632]).\n\tsize mismatch for base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\n\tsize mismatch for base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\n\tsize mismatch for base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 8192]) from checkpoint, the shape in current model is torch.Size([256, 5632]).\n\tsize mismatch for base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\n\tsize mismatch for base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\n\tsize mismatch for base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 8192]) from checkpoint, the shape in current model is torch.Size([256, 5632]).\n\tsize mismatch for base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\n\tsize mismatch for base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\n\tsize mismatch for base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 8192]) from checkpoint, the shape in current model is torch.Size([256, 5632]).\n\tsize mismatch for base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\n\tsize mismatch for base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\n\tsize mismatch for base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 8192]) from checkpoint, the shape in current model is torch.Size([256, 5632]).\n\tsize mismatch for base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\n\tsize mismatch for base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\n\tsize mismatch for base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 8192]) from checkpoint, the shape in current model is torch.Size([256, 5632]).\n\tsize mismatch for base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\n\tsize mismatch for base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\n\tsize mismatch for base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 8192]) from checkpoint, the shape in current model is torch.Size([256, 5632]).\n\tsize mismatch for base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\n\tsize mismatch for base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\n\tsize mismatch for base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 8192]) from checkpoint, the shape in current model is torch.Size([256, 5632]).\n\tsize mismatch for base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\n\tsize mismatch for base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\n\tsize mismatch for base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 8192]) from checkpoint, the shape in current model is torch.Size([256, 5632]).\n\tsize mismatch for base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\n\tsize mismatch for base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\n\tsize mismatch for base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 8192]) from checkpoint, the shape in current model is torch.Size([256, 5632]).\n\tsize mismatch for base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\n\tsize mismatch for base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\n\tsize mismatch for base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 8192]) from checkpoint, the shape in current model is torch.Size([256, 5632]).\n\tsize mismatch for base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\n\tsize mismatch for base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\n\tsize mismatch for base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 8192]) from checkpoint, the shape in current model is torch.Size([256, 5632]).\n\tsize mismatch for base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\n\tsize mismatch for base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\n\tsize mismatch for base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 8192]) from checkpoint, the shape in current model is torch.Size([256, 5632]).\n\tsize mismatch for base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\n\tsize mismatch for base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\n\tsize mismatch for base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 8192]) from checkpoint, the shape in current model is torch.Size([256, 5632]).\n\tsize mismatch for base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\n\tsize mismatch for base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\n\tsize mismatch for base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 8192]) from checkpoint, the shape in current model is torch.Size([256, 5632]).\n\tsize mismatch for base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.16.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.16.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\n\tsize mismatch for base_model.model.model.layers.16.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.16.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\n\tsize mismatch for base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 8192]) from checkpoint, the shape in current model is torch.Size([256, 5632]).\n\tsize mismatch for base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.17.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.17.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\n\tsize mismatch for base_model.model.model.layers.17.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.17.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\n\tsize mismatch for base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 8192]) from checkpoint, the shape in current model is torch.Size([256, 5632]).\n\tsize mismatch for base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.18.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.18.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\n\tsize mismatch for base_model.model.model.layers.18.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.18.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\n\tsize mismatch for base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 8192]) from checkpoint, the shape in current model is torch.Size([256, 5632]).\n\tsize mismatch for base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.19.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.19.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\n\tsize mismatch for base_model.model.model.layers.19.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.19.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\n\tsize mismatch for base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 8192]) from checkpoint, the shape in current model is torch.Size([256, 5632]).\n\tsize mismatch for base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.20.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.20.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\n\tsize mismatch for base_model.model.model.layers.20.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.20.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\n\tsize mismatch for base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 8192]) from checkpoint, the shape in current model is torch.Size([256, 5632]).\n\tsize mismatch for base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.21.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.21.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\n\tsize mismatch for base_model.model.model.layers.21.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.21.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\n\tsize mismatch for base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 8192]) from checkpoint, the shape in current model is torch.Size([256, 5632]).\n\tsize mismatch for base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m modal\u001b[38;5;241m.\u001b[39menable_output():\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m app\u001b[38;5;241m.\u001b[39mrun():\n\u001b[1;32m----> 3\u001b[0m         result\u001b[38;5;241m=\u001b[39m\u001b[43mprice\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremote\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mQuadcast HyperX condenser mic, connects via usb-c to your computer for crystal clear audio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m result\n",
      "File \u001b[1;32mc:\\Users\\pritam\\Desktop\\llm_engineering\\venv1\\lib\\site-packages\\synchronicity\\synchronizer.py:715\u001b[0m, in \u001b[0;36mSynchronizer._wrap_proxy_method.<locals>.proxy_method\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    713\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m suppress_synchronicity_tb_frames():\n\u001b[0;32m    714\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 715\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m wrapped_method(instance, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    716\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m UserCodeException \u001b[38;5;28;01mas\u001b[39;00m uc_exc:\n\u001b[0;32m    717\u001b[0m         uc_exc\u001b[38;5;241m.\u001b[39mexc\u001b[38;5;241m.\u001b[39m__suppress_context__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\pritam\\Desktop\\llm_engineering\\venv1\\lib\\site-packages\\synchronicity\\combined_types.py:33\u001b[0m, in \u001b[0;36mFunctionWithAio.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m UserCodeException \u001b[38;5;28;01mas\u001b[39;00m uc_exc:\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;66;03m# For Python < 3.11 we use UserCodeException as an exception wrapper\u001b[39;00m\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;66;03m# to remove some internal frames from tracebacks, but it can't remove\u001b[39;00m\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;66;03m# all frames\u001b[39;00m\n\u001b[0;32m     32\u001b[0m     uc_exc\u001b[38;5;241m.\u001b[39mexc\u001b[38;5;241m.\u001b[39m__suppress_context__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m uc_exc\u001b[38;5;241m.\u001b[39mexc\n",
      "File \u001b[1;32mc:\\Users\\pritam\\Desktop\\llm_engineering\\venv1\\lib\\site-packages\\modal\\_object.py:46\u001b[0m, in \u001b[0;36mlive_method.<locals>.wrapped\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(method)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhydrate()\n\u001b[1;32m---> 46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m method(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\pritam\\Desktop\\llm_engineering\\venv1\\lib\\site-packages\\modal\\_functions.py:1766\u001b[0m, in \u001b[0;36m_Function.remote\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1761\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_generator:\n\u001b[0;32m   1762\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidError(\n\u001b[0;32m   1763\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA generator function cannot be called with `.remote(...)`. Use `.remote_gen(...)` instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1764\u001b[0m     )\n\u001b[1;32m-> 1766\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_function(args, kwargs)\n",
      "File \u001b[1;32mc:\\Users\\pritam\\Desktop\\llm_engineering\\venv1\\lib\\site-packages\\modal\\_functions.py:1710\u001b[0m, in \u001b[0;36m_Function._call_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1701\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1702\u001b[0m     invocation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m _Invocation\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[0;32m   1703\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1704\u001b[0m         args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1707\u001b[0m         function_call_invocation_type\u001b[38;5;241m=\u001b[39mapi_pb2\u001b[38;5;241m.\u001b[39mFUNCTION_CALL_INVOCATION_TYPE_SYNC,\n\u001b[0;32m   1708\u001b[0m     )\n\u001b[1;32m-> 1710\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m invocation\u001b[38;5;241m.\u001b[39mrun_function()\n",
      "File \u001b[1;32mc:\\Users\\pritam\\Desktop\\llm_engineering\\venv1\\lib\\site-packages\\modal\\_functions.py:293\u001b[0m, in \u001b[0;36m_Invocation.run_function\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m ctx\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ctx\u001b[38;5;241m.\u001b[39mretry_policy\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    290\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ctx\u001b[38;5;241m.\u001b[39msync_client_retries_enabled\n\u001b[0;32m    291\u001b[0m ):\n\u001b[0;32m    292\u001b[0m     item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_single_output()\n\u001b[1;32m--> 293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m _process_result(item\u001b[38;5;241m.\u001b[39mresult, item\u001b[38;5;241m.\u001b[39mdata_format, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstub, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient)\n\u001b[0;32m    295\u001b[0m \u001b[38;5;66;03m# User errors including timeouts are managed by the user specified retry policy.\u001b[39;00m\n\u001b[0;32m    296\u001b[0m user_retry_manager \u001b[38;5;241m=\u001b[39m RetryManager(ctx\u001b[38;5;241m.\u001b[39mretry_policy)\n",
      "File \u001b[1;32mc:\\Users\\pritam\\Desktop\\llm_engineering\\venv1\\lib\\site-packages\\modal\\_utils\\function_utils.py:527\u001b[0m, in \u001b[0;36m_process_result\u001b[1;34m(result, data_format, stub, client)\u001b[0m\n\u001b[0;32m    524\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    525\u001b[0m                 \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m--> 527\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exc_with_hints(exc)\n\u001b[0;32m    529\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RemoteError(result\u001b[38;5;241m.\u001b[39mexception)\n\u001b[0;32m    531\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m<ta-01KHV15HRWRH2WF1ACWH86R3D9>:/root/pricer_ephemeral.py:54\u001b[0m, in \u001b[0;36mprice\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m<ta-01KHV15HRWRH2WF1ACWH86R3D9>:/usr/local/lib/python3.10/site-packages/peft/peft_model.py:568\u001b[0m, in \u001b[0;36mfrom_pretrained\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m<ta-01KHV15HRWRH2WF1ACWH86R3D9>:/usr/local/lib/python3.10/site-packages/peft/peft_model.py:1368\u001b[0m, in \u001b[0;36mload_adapter\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m<ta-01KHV15HRWRH2WF1ACWH86R3D9>:/usr/local/lib/python3.10/site-packages/peft/utils/save_and_load.py:565\u001b[0m, in \u001b[0;36mset_peft_model_state_dict\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m<ta-01KHV15HRWRH2WF1ACWH86R3D9>:/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:2635\u001b[0m, in \u001b[0;36mload_state_dict\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for PeftModelForCausalLM:\n\tsize mismatch for base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\n\tsize mismatch for base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\n\tsize mismatch for base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 8192]) from checkpoint, the shape in current model is torch.Size([256, 5632]).\n\tsize mismatch for base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\n\tsize mismatch for base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\n\tsize mismatch for base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 8192]) from checkpoint, the shape in current model is torch.Size([256, 5632]).\n\tsize mismatch for base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\n\tsize mismatch for base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\n\tsize mismatch for base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 8192]) from checkpoint, the shape in current model is torch.Size([256, 5632]).\n\tsize mismatch for base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\n\tsize mismatch for base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\n\tsize mismatch for base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 8192]) from checkpoint, the shape in current model is torch.Size([256, 5632]).\n\tsize mismatch for base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\n\tsize mismatch for base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\n\tsize mismatch for base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 8192]) from checkpoint, the shape in current model is torch.Size([256, 5632]).\n\tsize mismatch for base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\n\tsize mismatch for base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\n\tsize mismatch for base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 8192]) from checkpoint, the shape in current model is torch.Size([256, 5632]).\n\tsize mismatch for base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\n\tsize mismatch for base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\n\tsize mismatch for base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 8192]) from checkpoint, the shape in current model is torch.Size([256, 5632]).\n\tsize mismatch for base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\n\tsize mismatch for base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\n\tsize mismatch for base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 8192]) from checkpoint, the shape in current model is torch.Size([256, 5632]).\n\tsize mismatch for base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\n\tsize mismatch for base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\n\tsize mismatch for base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 8192]) from checkpoint, the shape in current model is torch.Size([256, 5632]).\n\tsize mismatch for base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\n\tsize mismatch for base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\n\tsize mismatch for base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 8192]) from checkpoint, the shape in current model is torch.Size([256, 5632]).\n\tsize mismatch for base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\n\tsize mismatch for base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\n\tsize mismatch for base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 8192]) from checkpoint, the shape in current model is torch.Size([256, 5632]).\n\tsize mismatch for base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\n\tsize mismatch for base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\n\tsize mismatch for base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 8192]) from checkpoint, the shape in current model is torch.Size([256, 5632]).\n\tsize mismatch for base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\n\tsize mismatch for base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\n\tsize mismatch for base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 8192]) from checkpoint, the shape in current model is torch.Size([256, 5632]).\n\tsize mismatch for base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\n\tsize mismatch for base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\n\tsize mismatch for base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 8192]) from checkpoint, the shape in current model is torch.Size([256, 5632]).\n\tsize mismatch for base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\n\tsize mismatch for base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\n\tsize mismatch for base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 8192]) from checkpoint, the shape in current model is torch.Size([256, 5632]).\n\tsize mismatch for base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\n\tsize mismatch for base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\n\tsize mismatch for base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 8192]) from checkpoint, the shape in current model is torch.Size([256, 5632]).\n\tsize mismatch for base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.16.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.16.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\n\tsize mismatch for base_model.model.model.layers.16.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.16.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\n\tsize mismatch for base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 8192]) from checkpoint, the shape in current model is torch.Size([256, 5632]).\n\tsize mismatch for base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.17.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.17.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\n\tsize mismatch for base_model.model.model.layers.17.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.17.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\n\tsize mismatch for base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 8192]) from checkpoint, the shape in current model is torch.Size([256, 5632]).\n\tsize mismatch for base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.18.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.18.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\n\tsize mismatch for base_model.model.model.layers.18.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.18.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\n\tsize mismatch for base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 8192]) from checkpoint, the shape in current model is torch.Size([256, 5632]).\n\tsize mismatch for base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.19.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.19.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\n\tsize mismatch for base_model.model.model.layers.19.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.19.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\n\tsize mismatch for base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 8192]) from checkpoint, the shape in current model is torch.Size([256, 5632]).\n\tsize mismatch for base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.20.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.20.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\n\tsize mismatch for base_model.model.model.layers.20.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.20.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\n\tsize mismatch for base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 8192]) from checkpoint, the shape in current model is torch.Size([256, 5632]).\n\tsize mismatch for base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n\tsize mismatch for base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256]).\n\tsize mismatch for base_model.model.model.layers.21.mlp.gate_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.21.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\n\tsize mismatch for base_model.model.model.layers.21.mlp.up_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 3072]) from checkpoint, the shape in current model is torch.Size([256, 2048]).\n\tsize mismatch for base_model.model.model.layers.21.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([8192, 256]) from checkpoint, the shape in current model is torch.Size([5632, 256]).\n\tsize mismatch for base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([256, 8192]) from checkpoint, the shape in current model is torch.Size([256, 5632]).\n\tsize mismatch for base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight: copying a param with shape torch.Size([3072, 256]) from checkpoint, the shape in current model is torch.Size([2048, 256])."
     ]
    }
   ],
   "source": [
    "with modal.enable_output():\n",
    "    with app.run():\n",
    "        result=price.remote(\"Quadcast HyperX condenser mic, connects via usb-c to your computer for crystal clear audio\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ee8e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = Preprocessor()\n",
    "text = preprocessor.preprocess(\"Quadcast HyperX condenser mic, connects via usb-c to your computer for crystal clear audio\")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9669ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = Preprocessor(model_name=\"groq/openai/gpt-oss-20b\")\n",
    "text = preprocessor.preprocess(\"Quadcast HyperX condenser mic, connects via usb-c to your computer for crystal clear audio\")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df6f295",
   "metadata": {},
   "source": [
    "### Add this to your .env if you want the Preprocessor to use a different model by default:\n",
    "\n",
    "`PRICER_PREPROCESSOR_MODEL=groq/openai/gpt-oss-20b`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb93e74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with modal.enable_output():\n",
    "    with app.run():\n",
    "        result = price.remote(text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca042a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d8747f-8452-4077-8af6-27e03888508a",
   "metadata": {},
   "source": [
    "## Transitioning From Ephemeral Apps to Deployed Apps\n",
    "\n",
    "From a command line, `uv run modal deploy xxx` will deploy your code as a Deployed App\n",
    "\n",
    "This is how you could package your AI service behind an API to be used in a Production System.\n",
    "\n",
    "You can also build REST endpoints easily, although we won't cover that as we'll be calling direct from Python.\n",
    "\n",
    "## Important note about secrets\n",
    "\n",
    "In both the files `pricer_service.py` and `pricer_service2.py` you will find code like this near the top:  \n",
    "`secrets = [modal.Secret.from_name(\"hf-secret\")]`  \n",
    "You may need to change from `hf-secret` to `huggingface-secret` depending on how the Secret is configured in modal.  \n",
    "To check, visit this page and look in the first column:  \n",
    "https://modal.com/secrets/\n",
    "\n",
    "## Important note for Windows people:\n",
    "\n",
    "On the next line, I call `uv run modal deploy` from within Jupyter lab; I've heard that on some versions of Windows this gives a strange unicode error because modal prints emojis to the output which can't be displayed. If that happens to you, open a Terminal and run `uv run modal deploy..`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f90d857-2f12-4521-bb90-28efd917f7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also run \"uv run modal deploy -m pricer_service\" in the Terminal\n",
    "\n",
    "!uv run modal deploy -m pricer_service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dec70ff-1986-4405-8624-9bbbe0ce1f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pricer = modal.Function.from_name(\"pricer-service\", \"price\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f443cc5",
   "metadata": {},
   "source": [
    "Watch it happening:\n",
    "\n",
    "https://modal.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17776139-0d9e-4ad0-bcd0-82d3a92ca61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This can take a while! We'll use faster approaches shortly\n",
    "\n",
    "pricer.remote(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56d1e55-2a03-4ce2-bb47-2ab6b9175a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also run \"modal deploy -m pricer_service2\" at the command line in an activated environment\n",
    "\n",
    "!modal deploy -m pricer_service2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e19daeb-1281-484b-9d2f-95cc6fed2622",
   "metadata": {},
   "outputs": [],
   "source": [
    "Pricer = modal.Cls.from_name(\"pricer-service\", \"Pricer\")\n",
    "pricer = Pricer()\n",
    "reply = pricer.price.remote(text)\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd499ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "reply = pricer.price.remote(text)\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1b1451-6249-4462-bf2d-5937c059926c",
   "metadata": {},
   "source": [
    "# Optional: Keeping Modal warm\n",
    "\n",
    "## A way to improve the speed of the Modal pricer service\n",
    "\n",
    "The first time you run this modal class, it might take as much as 10 minutes to build.  \n",
    "Subsequently it should be much faster.. 30 seconds if it needs to wake up, otherwise 2 seconds.  \n",
    "If you want it to always be 2 seconds, you can keep the container from going to sleep by editing this constant in pricer_service2.py:\n",
    "\n",
    "`MIN_CONTAINERS = 0`\n",
    "\n",
    "\n",
    "\n",
    "Make it 1 to keep a container alive.  \n",
    "But please note: this will eat up credits! Only do this if you are comfortable to have a process running continually.\n",
    "\n",
    "Alternatively, you can run this code and it will stay warm for 20 mins rather than 2 mins.\n",
    "\n",
    "### Code to keep warm for 20 mins before cooling down:\n",
    "\n",
    "```python\n",
    "import modal\n",
    "Pricer = modal.Cls.from_name(\"pricer-service\", \"Pricer\")\n",
    "pricer = Pricer()\n",
    "pricer.update_autoscaler(scaledown_window=1200)\n",
    "```\n",
    "\n",
    "### Code to revert to keeping warm for only 2 mins\n",
    "\n",
    "```python\n",
    "import modal\n",
    "Pricer = modal.Cls.from_name(\"pricer-service\", \"Pricer\")\n",
    "pricer = Pricer()\n",
    "pricer.update_autoscaler(scaledown_window=120)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3754cfdd-ae28-47c8-91f2-6e060e2c91b3",
   "metadata": {},
   "source": [
    "## And now introducing our Agent class\n",
    "\n",
    "By default this will preprocess using Llama3.2\n",
    "\n",
    "If you'd prefer to use Groq, then add this env variable like:\n",
    "\n",
    "```\n",
    "PRICER_PREPROCESSOR_MODEL=groq/openai/gpt-oss-20b\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cdc4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "root = logging.getLogger()\n",
    "root.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9aedca-6a7b-4d30-9f64-59d76f76fb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents.specialist_agent import SpecialistAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5843e5-e958-4a65-8326-8f5b4686de7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = SpecialistAgent()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70cd0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.price(\"iPhone 10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430869a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv1 (3.10.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
